{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a838e37",
   "metadata": {},
   "source": [
    "### CAG (Cache Augumented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82cbfc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "###simple cag implementation\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from typing import Dict\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model= init_chat_model(model=\"groq:llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a564ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var={}\n",
    "\n",
    "\n",
    "def model_cache(query:str)->str:\n",
    "    start_time = time.time()\n",
    "    if query in model_var.keys():\n",
    "        print(\"---CACHE EXECUTED---\")\n",
    "        ans = model_var[query]\n",
    "    else:\n",
    "        response = model.invoke(query)\n",
    "        model_var[query]=response\n",
    "        print(\"---RESPONSE DIRECTLY FROM MODEL--\")\n",
    "        ans = response\n",
    "    end_time = time.time()\n",
    "    print(f\"Total time to execute:\", round(end_time - start_time, 2))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "302bf888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RESPONSE DIRECTLY FROM MODEL--\n",
      "Total time to execute: 0.32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm doing well, thank you for asking. I'm a large language model, so I don't have emotions or feelings in the same way that humans do, but I'm functioning properly and ready to help with any questions or tasks you may have. How about you? How's your day going?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 41, 'total_tokens': 103, 'completion_time': 0.067249273, 'completion_tokens_details': None, 'prompt_time': 0.001913501, 'prompt_tokens_details': None, 'queue_time': 0.050889829, 'total_time': 0.069162774}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7688-f57a-7f22-b28f-34d54941a25f-0', usage_metadata={'input_tokens': 41, 'output_tokens': 62, 'total_tokens': 103})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=model_cache(query=\"hi, how are you doing\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28253ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CACHE EXECUTED---\n",
      "Total time to execute: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm doing well, thank you for asking. I'm a large language model, so I don't have emotions or feelings in the same way that humans do, but I'm functioning properly and ready to help with any questions or tasks you may have. How about you? How's your day going?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 41, 'total_tokens': 103, 'completion_time': 0.067249273, 'completion_tokens_details': None, 'prompt_time': 0.001913501, 'prompt_tokens_details': None, 'queue_time': 0.050889829, 'total_time': 0.069162774}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7688-f57a-7f22-b28f-34d54941a25f-0', usage_metadata={'input_tokens': 41, 'output_tokens': 62, 'total_tokens': 103})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_b=model_cache(query=\"hi, how are you doing\")\n",
    "result_b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGs In Depth (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
