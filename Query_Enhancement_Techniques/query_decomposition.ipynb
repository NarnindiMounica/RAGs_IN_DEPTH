{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e983dde6",
   "metadata": {},
   "source": [
    "### Query Decomposition:\n",
    "\n",
    "Query decomposition is the process of takinga complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "It is useful in\n",
    "* Complex queries often involve multiple concepts.\n",
    "* LLMs or retrievers may miss parts of the original question.\n",
    "* It enables multi-hop reasoning (answering in steps)\n",
    "* Allows parallelism(especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a25cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAGs In Depth\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9699b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Ingestion and Embedding Documents\n",
    "\n",
    "documents = TextLoader('langchain_crewai.txt', encoding=\"utf-8\").load()\n",
    "\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50).split_documents(documents)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore= FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "retriever= vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":4, \"lambda_mult\":0.7})\n",
    "\n",
    "#The lambda_mult parameter controls this trade-off:\n",
    "\n",
    "# lambda_mult = 1.0 → prioritize only relevance (like normal similarity search)\n",
    "# lambda_mult = 0.0 → prioritize only diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7f9d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001C9192FAF10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C96CE3CD50>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#llm model\n",
    "llm = init_chat_model(model=\"groq:llama-3.1-8b-instant\")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd5de23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='\\n    You are an AI Assistant. Decompose the following complex question into 2 or more sub-questions for better document retrieval.\\n    question: {question}\\n\\n    sub-question: \\n    ')\n",
       "| ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001C9192FAF10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C96CE3CD50>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query decomposition prompt\n",
    "\n",
    "decompose_prompt = PromptTemplate.from_template(\n",
    "    '''\n",
    "    You are an AI Assistant. Decompose the following complex question into 2 or more sub-questions for better document retrieval.\n",
    "    question: {question}\n",
    "\n",
    "    sub-question: \n",
    "    '''\n",
    ")\n",
    "\n",
    "decomposition_chain = decompose_prompt | llm | StrOutputParser()\n",
    "\n",
    "decomposition_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d93eaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To decompose the complex question into sub-questions for better document retrieval, I would suggest the following:\\n\\n1. **Langchain architecture**: What are the key components of the Langchain memory system?\\n2. **Langchain agents**: How do Langchain agents interact with the memory system to generate responses?\\n3. **CrewAI overview**: What are the primary differences between Langchain and CrewAI in terms of their architecture and approach?\\n4. **Memory usage comparison**: How does Langchain's memory usage compare to CrewAI's in terms of storage and retrieval?\\n5. **Agent comparison**: What are the key differences between Langchain's agents and CrewAI's agents in terms of their capabilities and functionality?\\n6. **Langchain vs CrewAI**: What are the primary advantages and disadvantages of Langchain's memory and agent systems compared to CrewAI's?\\n\\nBy breaking down the complex question into these sub-questions, we can retrieve more specific and relevant information from documents, ultimately providing a more comprehensive and accurate answer to the original question.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How does langchain use memory and agents compared to crewai?\"\n",
    "sub_questions= decomposition_chain.invoke({\"question\": query})\n",
    "sub_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "086bed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To decompose the complex question into sub-questions for better document retrieval, I would suggest the following:\n",
      "\n",
      "1. **Langchain architecture**: What are the key components of the Langchain memory system?\n",
      "2. **Langchain agents**: How do Langchain agents interact with the memory system to generate responses?\n",
      "3. **CrewAI overview**: What are the primary differences between Langchain and CrewAI in terms of their architecture and approach?\n",
      "4. **Memory usage comparison**: How does Langchain's memory usage compare to CrewAI's in terms of storage and retrieval?\n",
      "5. **Agent comparison**: What are the key differences between Langchain's agents and CrewAI's agents in terms of their capabilities and functionality?\n",
      "6. **Langchain vs CrewAI**: What are the primary advantages and disadvantages of Langchain's memory and agent systems compared to CrewAI's?\n",
      "\n",
      "By breaking down the complex question into these sub-questions, we can retrieve more specific and relevant information from documents, ultimately providing a more comprehensive and accurate answer to the original question.\n"
     ]
    }
   ],
   "source": [
    "print(sub_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e72e9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QA chain per sub_question\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(\n",
    "    ''' Use the context belwo to answer the question.\n",
    "\n",
    "    context: {context}\n",
    "\n",
    "    question: {input}\n",
    "    '''\n",
    ")\n",
    "\n",
    "documents_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b72fce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting each question from sub_questions\n",
    "import re\n",
    "questions=sub_questions.split(\"\\n\\n\")\n",
    "question=questions[1].split(\"\\n\")\n",
    "u=[]\n",
    "for q in question:\n",
    "    q = re.sub(r\"\\d+\\.\\s*\\*\\*.*?\\*\\*:\", \"\", q)\n",
    "    u.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c301eda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What are the key components of the Langchain memory system?\n",
      "Based on the provided context, the key components of the Langchain memory system include:\n",
      "\n",
      "1. Support for prompt templates: This allows agents to maintain a consistent tone and style in their responses.\n",
      "2. Memory: This enables agents to remember and reuse information from previous interactions, providing continuity and context to the conversation.\n",
      "\n",
      "These components are mentioned in the context of Document(id='a58203ca-bd87-4d5f-aa43-524342101bb1'). \n",
      " ****************************************************************************************************\n",
      "Question:  How do Langchain agents interact with the memory system to generate responses?\n",
      "Based on the provided context, it is not explicitly stated how Langchain agents interact with the memory system to generate responses. However, it can be inferred that Langchain's support for memory and prompt templates allows agents to maintain continuity and reuse.\n",
      "\n",
      "From the given information, we know that Langchain's chains are sequences of steps that can involve:\n",
      "\n",
      "1. Querying a model\n",
      "2. Manipulating data\n",
      "3. Calling external services\n",
      "\n",
      "It can be inferred that Langchain's memory system is likely used in conjunction with these steps to enable agents to maintain context and reuse previously obtained information when generating responses.\n",
      "\n",
      "Unfortunately, the exact mechanism of interaction between Langchain agents and the memory system is not explicitly described in the provided context. \n",
      " ****************************************************************************************************\n",
      "Question:  What are the primary differences between Langchain and CrewAI in terms of their architecture and approach?\n",
      "Based on the provided context, the primary differences between Langchain and CrewAI in terms of their architecture and approach are:\n",
      "\n",
      "1. **Infrastructure vs Orchestration**: Langchain provides the infrastructure for retrieval, prompt management, and tool integration, whereas CrewAI provides the orchestration layer that ensures multiple agents can collaborate effectively.\n",
      "\n",
      "2. **Scope of Functionality**: The two frameworks seem to have complementary scopes of functionality. Langchain focuses on the core infrastructure, while CrewAI handles the orchestration of multiple agents, enabling collaboration.\n",
      "\n",
      "3. **Approach to AI System Development**: Langchain aims to provide a more aligned, collaborative, and scalable AI system development experience, grounded in human values and needs. CrewAI, on the other hand, focuses on ensuring effective collaboration between multiple agents, which is essential for building such systems.\n",
      "\n",
      "In summary, Langchain provides the foundational infrastructure, while CrewAI adds the layer of orchestration, enabling multiple agents to work together effectively. \n",
      " ****************************************************************************************************\n",
      "Question:  How does Langchain's memory usage compare to CrewAI's in terms of storage and retrieval?\n",
      "Unfortunately, the provided context does not mention anything about LangChain's memory usage or storage and retrieval capabilities compared to CrewAI. The context discusses the synergy and complementary aspects of LangChain and CrewAI, but does not provide information about their memory usage. \n",
      "\n",
      "However, based on the information provided in the context, we can infer that LangChain provides infrastructure for retrieval and grounding, which suggests that it has capabilities for storing and retrieving information. On the other hand, CrewAI is mentioned as coordinating agents, which implies that it might have different storage and retrieval requirements compared to LangChain.\n",
      "\n",
      "To answer the question accurately, further information about LangChain's and CrewAI's memory usage would be needed. \n",
      " ****************************************************************************************************\n",
      "Question:  What are the key differences between Langchain's agents and CrewAI's agents in terms of their capabilities and functionality?\n",
      "Based on the provided context, it appears that LangChain and CrewAI are complementary frameworks. \n",
      "\n",
      "LangChain seems to provide the infrastructure for:\n",
      "\n",
      "1. Retrieval\n",
      "2. Prompt management\n",
      "3. Tool integration\n",
      "\n",
      "CrewAI, on the other hand, provides the orchestration layer that allows multiple agents to collaborate effectively. \n",
      "\n",
      "While the context does not explicitly state the key differences in terms of capabilities and functionality between LangChain's agents and CrewAI's agents, it can be inferred that:\n",
      "\n",
      "- LangChain's agents are focused on individual tasks such as retrieval, while CrewAI's agents are capable of collaborating with each other to deliver comprehensive outputs.\n",
      "- LangChain might provide the backbone for retrieval tasks, while each CrewAI agent can specialize in a different domain, working together to achieve a common goal.\n",
      "\n",
      "In summary, the key differences between LangChain's agents and CrewAI's agents lie in their scope of functionality and their ability to collaborate. \n",
      " ****************************************************************************************************\n",
      "Question:  What are the primary advantages and disadvantages of Langchain's memory and agent systems compared to CrewAI's?\n",
      "Based on the provided context, it seems that LangChain and CrewAI are being compared as two separate systems. However, there is limited information available about LangChain's memory and agent systems. \n",
      "\n",
      "From the given context, we can infer the following:\n",
      "\n",
      "Advantages of CrewAI's system:\n",
      "\n",
      "1. Ensures LLMs are grounded in external knowledge, preventing \"hallucinating\" answers.\n",
      "2. Emphasizes multi-agent collaboration, allowing tasks to be broken down and tackled.\n",
      "\n",
      "Disadvantages of CrewAI's system (not explicitly stated, but inferred from the context):\n",
      "\n",
      "1. May not provide the infrastructure for connecting LLMs to data and tools.\n",
      "\n",
      "Advantages of LangChain's system (not explicitly stated, but inferred from the context):\n",
      "\n",
      "1. Provides the infrastructure for connecting LLMs to data and tools.\n",
      "2. Handles the \"nuts and bolts\" of connecting to models and managing retrieval or prompt management.\n",
      "\n",
      "Disadvantages of LangChain's system (not explicitly stated, but inferred from the context):\n",
      "\n",
      "1. May not provide the orchestration for coordinating multiple agents.\n",
      "2. May not ensure that LLMs are grounded in external knowledge, potentially leading to \"hallucinating\" answers.\n",
      "\n",
      "It's essential to note that these inferences are based on a limited context and might not be entirely accurate. For a more comprehensive understanding of LangChain's memory and agent systems, additional information would be required. \n",
      " ****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "for q in u:\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"input\": RunnablePassthrough()}\n",
    "        | qa_prompt\n",
    "        | llm\n",
    "        | StrOutputParser())\n",
    "    response = rag_chain.invoke(q)\n",
    "    print(f\"Question: {q}\")\n",
    "    print(response, \"\\n\", \"*\"*100)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1433b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGs In Depth (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
