{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e983dde6",
   "metadata": {},
   "source": [
    "### Query Decomposition:\n",
    "\n",
    "Query decomposition is the process of takinga complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "It is useful in\n",
    "* Complex queries often involve multiple concepts.\n",
    "* LLMs or retrievers may miss parts of the original question.\n",
    "* It enables multi-hop reasoning (answering in steps)\n",
    "* Allows parallelism(especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a25cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAGs In Depth\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9699b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Ingestion and Embedding Documents\n",
    "\n",
    "documents = TextLoader('langchain_crewai.txt', encoding=\"utf-8\").load()\n",
    "\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50).split_documents(documents)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore= FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "retriever= vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":4, \"lambda_mult\":0.7})\n",
    "\n",
    "#The lambda_mult parameter controls this trade-off:\n",
    "\n",
    "# lambda_mult = 1.0 → prioritize only relevance (like normal similarity search)\n",
    "# lambda_mult = 0.0 → prioritize only diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7f9d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001C9192FAF10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C96CE3CD50>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#llm model\n",
    "llm = init_chat_model(model=\"groq:llama-3.1-8b-instant\")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd5de23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='\\n    You are an AI Assistant. Decompose the following complex question into 2 or more sub-questions for better document retrieval.\\n    question: {question}\\n\\n    sub-question: \\n    ')\n",
       "| ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001C9192FAF10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C96CE3CD50>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query decomposition prompt\n",
    "\n",
    "decompose_prompt = PromptTemplate.from_template(\n",
    "    '''\n",
    "    You are an AI Assistant. Decompose the following complex question into 2 or more sub-questions for better document retrieval.\n",
    "    question: {question}\n",
    "\n",
    "    sub-question: \n",
    "    '''\n",
    ")\n",
    "\n",
    "decomposition_chain = decompose_prompt | llm | StrOutputParser()\n",
    "\n",
    "decomposition_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d93eaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To decompose the complex question into sub-questions for better document retrieval, I would suggest the following:\\n\\n1. **Langchain architecture**: What are the key components of the Langchain memory system?\\n2. **Langchain agents**: How do Langchain agents interact with the memory system to generate responses?\\n3. **CrewAI overview**: What are the primary differences between Langchain and CrewAI in terms of their architecture and approach?\\n4. **Memory usage comparison**: How does Langchain's memory usage compare to CrewAI's in terms of storage and retrieval?\\n5. **Agent comparison**: What are the key differences between Langchain's agents and CrewAI's agents in terms of their capabilities and functionality?\\n6. **Langchain vs CrewAI**: What are the primary advantages and disadvantages of Langchain's memory and agent systems compared to CrewAI's?\\n\\nBy breaking down the complex question into these sub-questions, we can retrieve more specific and relevant information from documents, ultimately providing a more comprehensive and accurate answer to the original question.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How does langchain use memory and agents compared to crewai?\"\n",
    "sub_questions= decomposition_chain.invoke({\"question\": query})\n",
    "sub_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "086bed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To decompose the complex question into sub-questions for better document retrieval, I would suggest the following:\n",
      "\n",
      "1. **Langchain architecture**: What are the key components of the Langchain memory system?\n",
      "2. **Langchain agents**: How do Langchain agents interact with the memory system to generate responses?\n",
      "3. **CrewAI overview**: What are the primary differences between Langchain and CrewAI in terms of their architecture and approach?\n",
      "4. **Memory usage comparison**: How does Langchain's memory usage compare to CrewAI's in terms of storage and retrieval?\n",
      "5. **Agent comparison**: What are the key differences between Langchain's agents and CrewAI's agents in terms of their capabilities and functionality?\n",
      "6. **Langchain vs CrewAI**: What are the primary advantages and disadvantages of Langchain's memory and agent systems compared to CrewAI's?\n",
      "\n",
      "By breaking down the complex question into these sub-questions, we can retrieve more specific and relevant information from documents, ultimately providing a more comprehensive and accurate answer to the original question.\n"
     ]
    }
   ],
   "source": [
    "print(sub_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e72e9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QA chain per sub_question\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(\n",
    "    ''' Use the context belwo to answer the question.\n",
    "\n",
    "    context: {context}\n",
    "\n",
    "    question: {input}\n",
    "    '''\n",
    ")\n",
    "\n",
    "documents_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b72fce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting each question from sub_questions\n",
    "import re\n",
    "questions=sub_questions.split(\"\\n\\n\")\n",
    "question=questions[1].split(\"\\n\")\n",
    "u=[]\n",
    "for q in question:\n",
    "    q = re.sub(r\"\\d+\\.\\s*\\*\\*.*?\\*\\*:\", \"\", q)\n",
    "    u.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c301eda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What are the key components of the Langchain memory system?\n",
      "Based on the provided context, the key components of the Langchain memory system mentioned are:\n",
      "\n",
      "1. **Prompt templates**: These are likely pre-defined or customizable templates for generating prompts to be used by the Langchain system.\n",
      "2. **Memory**: This is a component that allows agents to maintain continuity and reuse information, which implies persisting data across interactions or sessions.\n",
      "\n",
      "These components are mentioned in the document with the id 'a58203ca-bd87-4d5f-aa43-524342101bb1', specifically in the page content 'LangChain’s support for prompt templates and memory means that agents can maintain continuity and reuse'. \n",
      " ********************************************************************************\n",
      "Question:  How do Langchain agents interact with the memory system to generate responses?\n",
      "Based on the provided context, the interaction between LangChain agents and the memory system is not explicitly mentioned. However, it can be inferred that LangChain's support for \"memory\" enables agents to maintain continuity and reuse information.\n",
      "\n",
      "From the context, it can be gathered that LangChain's chains are sequences of steps that can involve manipulating data. This suggests that the memory system is used to store and retrieve data that can be used by the agents to generate responses.\n",
      "\n",
      "Therefore, it can be inferred that LangChain agents interact with the memory system by:\n",
      "\n",
      "1. Accessing and manipulating stored data (i.e., manipulating data) as part of the chain of steps.\n",
      "2. Using the data stored in the memory system to generate responses.\n",
      "\n",
      "However, the exact mechanism of how LangChain agents interact with the memory system is not explicitly stated in the provided context. \n",
      " ********************************************************************************\n",
      "Question:  What are the primary differences between Langchain and CrewAI in terms of their architecture and approach?\n",
      "Based on the provided context, it seems that LangChain and CrewAI are complementary frameworks that together enable developers to build AI systems. While the context does not explicitly state the primary differences in their architecture and approach, it implies that:\n",
      "\n",
      "- LangChain provides the infrastructure for retrieval, prompt management, and tool integration. This suggests that LangChain handles the foundational components necessary for AI systems, such as data retrieval and prompt handling.\n",
      "\n",
      "- CrewAI provides the orchestration layer that ensures multiple agents can collaborate effectively. This implies that CrewAI focuses on the high-level coordination and management of AI systems, enabling them to work together seamlessly, much like a human team.\n",
      "\n",
      "In summary, the primary differences between LangChain and CrewAI seem to be:\n",
      "\n",
      "1. **Infrastructure vs. Orchestration**: LangChain provides the infrastructure for AI systems, while CrewAI provides the orchestration layer that enables collaboration between multiple agents.\n",
      "\n",
      "2. **Foundational components vs. High-level coordination**: LangChain handles the foundational components necessary for AI systems, such as data retrieval and prompt handling, while CrewAI focuses on high-level coordination and management.\n",
      "\n",
      "These differences suggest that LangChain and CrewAI are designed to work together, with LangChain providing the building blocks for AI systems and CrewAI ensuring that these systems can work together effectively. \n",
      " ********************************************************************************\n",
      "Question:  How does Langchain's memory usage compare to CrewAI's in terms of storage and retrieval?\n",
      "Based on the provided context, it's not possible to directly compare Langchain's memory usage with CrewAI's in terms of storage and retrieval. The context only mentions the functionalities and purposes of both systems but doesn't provide any information regarding memory usage.\n",
      "\n",
      "However, Langchain is mentioned as 'providing the infrastructure for retrieval,' and 'focus on retrieval and grounding.' This suggests that Langchain is more focused on data retrieval and retrieval-related tasks.\n",
      "\n",
      "On the other hand, CrewAI is mentioned as 'coordinating agents specializing in diagnosis, treatment planning, and patient,' which implies that CrewAI is more focused on coordination and integration of various tasks, possibly involving data analysis, decision-making, and other aspects.\n",
      "\n",
      "Given the context, without any specific data or metrics, it's not possible to make a direct comparison between Langchain's and CrewAI's memory usage. \n",
      " ********************************************************************************\n",
      "Question:  What are the key differences between Langchain's agents and CrewAI's agents in terms of their capabilities and functionality?\n",
      "Based on the provided context, the key differences between Langchain's agents and CrewAI's agents can be inferred as follows:\n",
      "\n",
      "1. Infrastructure: LangChain provides the infrastructure for retrieval, prompt management, and tool integration, whereas CrewAI provides the orchestration layer that ensures multiple agents can collaborate effectively.\n",
      "\n",
      "2. Specialization: CrewAI agents can specialize in different domains, whereas LangChain provides a retrieval backbone that can be used by various agents, possibly with different specialized functions.\n",
      "\n",
      "3. Collaboration: CrewAI is designed to enable the collaboration of multiple agents, whereas LangChain provides a foundational infrastructure for individual agents to operate within.\n",
      "\n",
      "4. Synergy: While CrewAI is designed to orchestrate multiple agents, LangChain can be seen as a key component in this orchestration, providing the retrieval backbone that allows agents to work together effectively.\n",
      "\n",
      "In summary, the key differences between Langchain's agents and CrewAI's agents lie in their capabilities, with LangChain focusing on providing infrastructure and CrewAI focusing on enabling the orchestration of multiple agents. \n",
      " ********************************************************************************\n",
      "Question:  What are the primary advantages and disadvantages of Langchain's memory and agent systems compared to CrewAI's?\n",
      "Based on the provided context, there is no information about the memory and agent systems of Langchain compared to CrewAI's systems. However, we can infer some information about their respective capabilities:\n",
      "\n",
      "- **Inferring Langchain's capabilities:**\n",
      "  - LangChain provides the infrastructure for retrieval and connecting LLMs to data and tools.\n",
      "  - It handles the nuts and bolts of connecting to models, which can be seen as low-level details.\n",
      "\n",
      "- **Inferring CrewAI's capabilities:**\n",
      "  - CrewAI provides the orchestration for coordinating multiple agents.\n",
      "  - It ensures that tasks can be broken down and tackled in a multi-agent collaboration environment.\n",
      "  - CrewAI emphasizes multi-agent collaboration and external knowledge grounding in LLMs to prevent \"hallucinating\" answers.\n",
      "\n",
      "However, since the question explicitly asks for the comparison of Langchain's memory and agent systems with CrewAI's, and since the provided context doesn't directly mention Langchain's memory and agent systems, I must conclude that the question cannot be fully answered based on the given context.\n",
      "\n",
      "If we were to speculate, we could say that CrewAI provides more advanced collaboration capabilities than Langchain's systems, but this would be purely based on inference and might not be accurate.\n",
      "\n",
      "To give a more accurate answer, we would need more information about Langchain's memory and agent systems, which is not provided in the context. \n",
      " ********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "for q in u:\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"input\": RunnablePassthrough()}\n",
    "        | qa_prompt\n",
    "        | llm\n",
    "        | StrOutputParser())\n",
    "    response = rag_chain.invoke(q)\n",
    "    print(f\"Question: {q}\")\n",
    "    print(response, \"\\n\", \"*\"*80)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1433b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGs In Depth (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
