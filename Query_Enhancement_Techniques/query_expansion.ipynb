{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8facb1db",
   "metadata": {},
   "source": [
    "### Query Expansion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "adbe1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96c1a2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ingested successfully\n",
      "Number of chunks: 62\n"
     ]
    }
   ],
   "source": [
    "#ingesting data and dividing into chunks for context window\n",
    "\n",
    "documents = TextLoader(file_path='langchain_crewai.txt', encoding=\"utf-8\").load()\n",
    "print(f\"Data is ingested successfully\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bd29cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001366D75E350>, search_type='mmr', search_kwargs={})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embedding model and vectorstore retriever\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "retriever = FAISS.from_documents(documents=chunks,\n",
    "                                 embedding=embedding_model).as_retriever(search_type='mmr', serach_kargs={\"k\":4})\n",
    "\n",
    "retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e1ea9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001366D97EC90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001366D9093D0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model\n",
    "\n",
    "model = init_chat_model(model=\"groq:llama-3.1-8b-instant\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f57baaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template=' given a query, expand the following query to improve document retrieval by adding relevant words, synonyms or technical terms.\\n    only provide enhanced query, no explanation needed\\n    query: {query}\\n    enhanced_query:\\n    ')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query expansion prompt\n",
    "\n",
    "query_expansion_prompt= PromptTemplate.from_template(\n",
    "    ''' given a query, expand the following query to improve document retrieval by adding relevant words, synonyms or technical terms.\n",
    "    only provide enhanced query, no explanation needed\n",
    "    query: {query}\n",
    "    enhanced_query:\n",
    "    '''\n",
    ")\n",
    "\n",
    "query_expansion_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "adc21fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain memory model implementation natural language processing knowledge graph semantic search.\n"
     ]
    }
   ],
   "source": [
    "query_enhance_chain = query_expansion_prompt | model | StrOutputParser()\n",
    "result=query_enhance_chain.invoke({\"query\": \"LangChain memory\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e7ab6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=\"Answer user's question based on  only given context information.\\n    context: {context}\\n    question: {input}\\n    \")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#qa prompt template to get context information\n",
    "qa_prompt = PromptTemplate.from_template(\n",
    "    '''Answer user's question based on  only given context information.\n",
    "    context: {context}\n",
    "    question: {input}\n",
    "    '''\n",
    ")\n",
    "\n",
    "qa_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04c0b453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=\"Answer user's question based on  only given context information.\\n    context: {context}\\n    question: {input}\\n    \")\n",
       "| ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001366D97EC90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001366D9093D0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating stuff documents chain\n",
    "\n",
    "documents_chain = create_stuff_documents_chain(llm=model,\n",
    "                                               prompt=qa_prompt)\n",
    "documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5fb26ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  input: RunnableLambda(...),\n",
       "  context: RunnableLambda(...)\n",
       "}\n",
       "| RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "    context: RunnableLambda(format_docs)\n",
       "  }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "  | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=\"Answer user's question based on  only given context information.\\n    context: {context}\\n    question: {input}\\n    \")\n",
       "  | ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001366D97EC90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001366D9093D0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "  | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating retrieval chain with query expansion\n",
    "\n",
    "rag_pipeline = (\n",
    "    RunnableMap( {\"input\" : lambda x : x['input'],\n",
    "                  \"context\": lambda x : retriever.invoke(query_enhance_chain.invoke({\"query\": x['input']}))\n",
    "    }\n",
    "    )\n",
    "| documents_chain\n",
    ")\n",
    "\n",
    "\n",
    "rag_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query run\n",
    "\n",
    "query= {\"input\": \" compare langchain and crewai\"}\n",
    "print(query_enhance_chain.invoke({\"query\": query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"Answer: \\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e0d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGs In Depth (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
