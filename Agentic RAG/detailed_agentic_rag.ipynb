{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c21ca5b",
   "metadata": {},
   "source": [
    "### Detailed Agentic RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72122c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAGs In Depth\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d05df",
   "metadata": {},
   "source": [
    "##### Langgraph documents retriever tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f601648f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='LangGraph overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageGain control with LangGraph to design agents that reliably handle complex tasksCopy pageTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopypip install -U langgraph\\n\\nThen, create a simple hello world example:\\nCopyfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\nDurable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\nLangSmithTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.Learn moreLangGraphDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.Learn moreLangChainProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.Learn more\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangGraphNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Workflows and agents - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedWorkflows and agentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in LangGraphEvaluator-optimizerAgentsGet startedWorkflows and agentsCopy pageCopy pageThis guide reviews common workflow and agent patterns.\\n\\nWorkflows have predetermined code paths and are designed to operate in a certain order.\\nAgents are dynamic and define their own processes and tool usage.\\n\\n\\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment.\\n\\u200bSetup\\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\\n\\nInstall dependencies:\\n\\nCopypip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize the LLM:\\n\\nCopyimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\\n\\n\\u200bLLMs and augmentations\\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.\\n\\nCopy# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\n\\u200bPrompt chaining\\nPrompt chaining is when each LLM call processes the output of the previous call. It’s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\\n\\nTranslating documents into different languages\\nVerifying generated content for consistency\\n\\n\\nGraph APIFunctional APICopyfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Final joke:\")\\n    print(state[\"joke\"])\\n\\n\\u200bParallelization\\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\\n\\nSplit up subtasks and run them in parallel, which increases speed\\nRun tasks multiple times to check for different outputs, which increases confidence\\n\\nSome examples include:\\n\\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\\n\\n\\nGraph APIFunctional APICopy# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\n\\u200bRouting\\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\\n\\nGraph APIFunctional APICopyfrom typing_extensions import Literal\\nfrom langchain.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\n\\u200bOrchestrator-worker\\nIn an orchestrator-worker configuration, the orchestrator:\\n\\nBreaks down tasks into subtasks\\nDelegates subtasks to workers\\nSynthesizes worker outputs into a final result\\n\\n\\nOrchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\\nGraph APIFunctional APICopyfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\u200bCreating workers in LangGraph\\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The Send API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send API to send a section to each worker.\\nCopyfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\n\\u200bEvaluator-optimizer\\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\\nEvaluator-optimizer workflows are commonly used when there’s particular success criteria for a task, but iteration is required to meet that criteria. For example, there’s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\\n\\nGraph APIFunctional APICopy# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\n\\u200bAgents\\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\\n\\nTo get started with agents, see the quickstart or read more about how they work in LangChain.\\nUsing toolsCopyfrom langchain.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional APICopyfrom langgraph.graph import MessagesState\\nfrom langchain.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoThinking in LangGraphPreviousPersistenceNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='Interrupts - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesInterruptsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pagePause using interruptResuming interruptsCommon patternsApprove or rejectReview and edit stateInterrupts in toolsValidating human inputRules of interruptsDo not wrap interrupt calls in try/exceptDo not reorder interrupt calls within a nodeDo not return complex values in interrupt callsSide effects called before interrupt must be idempotentUsing with subgraphs called as functionsDebugging with interruptsUsing LangGraph StudioCapabilitiesInterruptsCopy pageCopy pageInterrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its persistence layer and waits indefinitely until you resume execution.\\nInterrupts work by calling the interrupt() function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you’re ready to continue, you resume execution by re-invoking the graph using Command, which then becomes the return value of the interrupt() call from inside the node.\\nUnlike static breakpoints (which pause before or after specific nodes), interrupts are dynamic—they can be placed anywhere in your code and can be conditional based on your application logic.\\n\\nCheckpointing keeps your place: the checkpointer writes the exact graph state so you can resume later, even when in an error state.\\nthread_id is your pointer: set config={\"configurable\": {\"thread_id\": ...}} to tell the checkpointer which state to load.\\nInterrupt payloads surface as __interrupt__: the values you pass to interrupt() return to the caller in the __interrupt__ field so you know what the graph is waiting on.\\n\\nThe thread_id you choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state.\\n\\u200bPause using interrupt\\nThe interrupt function pauses graph execution and returns a value to the caller. When you call interrupt within a node, LangGraph saves the current graph state and waits for you to resume execution with input.\\nTo use interrupt, you need:\\n\\nA checkpointer to persist the graph state (use a durable checkpointer in production)\\nA thread ID in your config so the runtime knows which state to resume from\\nTo call interrupt() where you want to pause (payload must be JSON-serializable)\\n\\nCopyfrom langgraph.types import interrupt\\n\\ndef approval_node(state: State):\\n    # Pause and ask for approval\\n    approved = interrupt(\"Do you approve this action?\")\\n\\n    # When you resume, Command(resume=...) returns that value here\\n    return {\"approved\": approved}\\n\\nWhen you call interrupt, here’s what happens:\\n\\nGraph execution gets suspended at the exact point where interrupt is called\\nState is saved using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)\\nValue is returned to the caller under __interrupt__; it can be any JSON-serializable value (string, object, array, etc.)\\nGraph waits indefinitely until you resume execution with a response\\nResponse is passed back into the node when you resume, becoming the return value of the interrupt() call\\n\\n\\u200bResuming interrupts\\nAfter an interrupt pauses execution, you resume the graph by invoking it again with a Command that contains the resume value. The resume value is passed back to the interrupt call, allowing the node to continue execution with the external input.\\nCopyfrom langgraph.types import Command\\n\\n# Initial run - hits the interrupt and pauses\\n# thread_id is the persistent pointer (stores a stable ID in production)\\nconfig = {\"configurable\": {\"thread_id\": \"thread-1\"}}\\nresult = graph.invoke({\"input\": \"data\"}, config=config)\\n\\n# Check what was interrupted\\n# __interrupt__ contains the payload that was passed to interrupt()\\nprint(result[\"__interrupt__\"])\\n# > [Interrupt(value=\\'Do you approve this action?\\')]\\n\\n# Resume with the human\\'s response\\n# The resume payload becomes the return value of interrupt() inside the node\\ngraph.invoke(Command(resume=True), config=config)\\n\\nKey points about resuming:\\n\\nYou must use the same thread ID when resuming that was used when the interrupt occurred\\nThe value passed to Command(resume=...) becomes the return value of the interrupt call\\nThe node restarts from the beginning of the node where the interrupt was called when resumed, so any code before the interrupt runs again\\nYou can pass any JSON-serializable value as the resume value\\n\\n\\u200bCommon patterns\\nThe key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:\\n\\n Approval workflows: Pause before executing critical actions (API calls, database changes, financial transactions)\\n Review and edit: Let humans review and modify LLM outputs or tool calls before continuing\\n Interrupting tool calls: Pause before executing tool calls to review and edit the tool call before execution\\n Validating human input: Pause before proceeding to the next step to validate human input\\n\\n\\u200bApprove or reject\\nOne of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.\\nCopyfrom typing import Literal\\nfrom langgraph.types import interrupt, Command\\n\\ndef approval_node(state: State) -> Command[Literal[\"proceed\", \"cancel\"]]:\\n    # Pause execution; payload shows up under result[\"__interrupt__\"]\\n    is_approved = interrupt({\\n        \"question\": \"Do you want to proceed with this action?\",\\n        \"details\": state[\"action_details\"]\\n    })\\n\\n    # Route based on the response\\n    if is_approved:\\n        return Command(goto=\"proceed\")  # Runs after the resume payload is provided\\n    else:\\n        return Command(goto=\"cancel\")\\n\\nWhen you resume the graph, pass true to approve or false to reject:\\nCopy# To approve\\ngraph.invoke(Command(resume=True), config=config)\\n\\n# To reject\\ngraph.invoke(Command(resume=False), config=config)\\n\\nFull exampleCopyfrom typing import Literal, Optional, TypedDict\\n\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Command, interrupt\\n\\n\\nclass ApprovalState(TypedDict):\\n    action_details: str\\n    status: Optional[Literal[\"pending\", \"approved\", \"rejected\"]]\\n\\n\\ndef approval_node(state: ApprovalState) -> Command[Literal[\"proceed\", \"cancel\"]]:\\n    # Expose details so the caller can render them in a UI\\n    decision = interrupt({\\n        \"question\": \"Approve this action?\",\\n        \"details\": state[\"action_details\"],\\n    })\\n\\n    # Route to the appropriate node after resume\\n    return Command(goto=\"proceed\" if decision else \"cancel\")\\n\\n\\ndef proceed_node(state: ApprovalState):\\n    return {\"status\": \"approved\"}\\n\\n\\ndef cancel_node(state: ApprovalState):\\n    return {\"status\": \"rejected\"}\\n\\n\\nbuilder = StateGraph(ApprovalState)\\nbuilder.add_node(\"approval\", approval_node)\\nbuilder.add_node(\"proceed\", proceed_node)\\nbuilder.add_node(\"cancel\", cancel_node)\\nbuilder.add_edge(START, \"approval\")\\nbuilder.add_edge(\"proceed\", END)\\nbuilder.add_edge(\"cancel\", END)\\n\\n# Use a more durable checkpointer in production\\ncheckpointer = MemorySaver()\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"approval-123\"}}\\ninitial = graph.invoke(\\n    {\"action_details\": \"Transfer $500\", \"status\": \"pending\"},\\n    config=config,\\n)\\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={\\'question\\': ..., \\'details\\': ...})]\\n\\n# Resume with the decision; True routes to proceed, False to cancel\\nresumed = graph.invoke(Command(resume=True), config=config)\\nprint(resumed[\"status\"])  # -> \"approved\"\\n\\n\\u200bReview and edit state\\nSometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.\\nCopyfrom langgraph.types import interrupt\\n\\ndef review_node(state: State):\\n    # Pause and show the current content for review (surfaces in result[\"__interrupt__\"])\\n    edited_content = interrupt({\\n        \"instruction\": \"Review and edit this content\",\\n        \"content\": state[\"generated_text\"]\\n    })\\n\\n    # Update the state with the edited version\\n    return {\"generated_text\": edited_content}\\n\\nWhen resuming, provide the edited content:\\nCopygraph.invoke(\\n    Command(resume=\"The edited and improved text\"),  # Value becomes the return from interrupt()\\n    config=config\\n)\\n\\nFull exampleCopyimport sqlite3\\nfrom typing import TypedDict\\n\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Command, interrupt\\n\\n\\nclass ReviewState(TypedDict):\\n    generated_text: str\\n\\n\\ndef review_node(state: ReviewState):\\n    # Ask a reviewer to edit the generated content\\n    updated = interrupt({\\n        \"instruction\": \"Review and edit this content\",\\n        \"content\": state[\"generated_text\"],\\n    })\\n    return {\"generated_text\": updated}\\n\\n\\nbuilder = StateGraph(ReviewState)\\nbuilder.add_node(\"review\", review_node)\\nbuilder.add_edge(START, \"review\")\\nbuilder.add_edge(\"review\", END)\\n\\ncheckpointer = MemorySaver()\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"review-42\"}}\\ninitial = graph.invoke({\"generated_text\": \"Initial draft\"}, config=config)\\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={\\'instruction\\': ..., \\'content\\': ...})]\\n\\n# Resume with the edited text from the reviewer\\nfinal_state = graph.invoke(\\n    Command(resume=\"Improved draft after review\"),\\n    config=config,\\n)\\nprint(final_state[\"generated_text\"])  # -> \"Improved draft after review\"\\n\\n\\u200bInterrupts in tools\\nYou can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it’s called, and allows for human review and editing of the tool call before it is executed.\\nFirst, define a tool that uses interrupt:\\nCopyfrom langchain.tools import tool\\nfrom langgraph.types import interrupt\\n\\n@tool\\ndef send_email(to: str, subject: str, body: str):\\n    \"\"\"Send an email to a recipient.\"\"\"\\n\\n    # Pause before sending; payload surfaces in result[\"__interrupt__\"]\\n    response = interrupt({\\n        \"action\": \"send_email\",\\n        \"to\": to,\\n        \"subject\": subject,\\n        \"body\": body,\\n        \"message\": \"Approve sending this email?\"\\n    })\\n\\n    if response.get(\"action\") == \"approve\":\\n        # Resume value can override inputs before executing\\n        final_to = response.get(\"to\", to)\\n        final_subject = response.get(\"subject\", subject)\\n        final_body = response.get(\"body\", body)\\n        return f\"Email sent to {final_to} with subject \\'{final_subject}\\'\"\\n    return \"Email cancelled by user\"\\n\\nThis approach is useful when you want the approval logic to live with the tool itself, making it reusable across different parts of your graph. The LLM can call the tool naturally, and the interrupt will pause execution whenever the tool is invoked, allowing you to approve, edit, or cancel the action.\\nFull exampleCopyimport sqlite3\\nfrom typing import TypedDict\\n\\nfrom langchain.tools import tool\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langgraph.checkpoint.sqlite import SqliteSaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Command, interrupt\\n\\n\\nclass AgentState(TypedDict):\\n    messages: list[dict]\\n\\n\\n@tool\\ndef send_email(to: str, subject: str, body: str):\\n    \"\"\"Send an email to a recipient.\"\"\"\\n\\n    # Pause before sending; payload surfaces in result[\"__interrupt__\"]\\n    response = interrupt({\\n        \"action\": \"send_email\",\\n        \"to\": to,\\n        \"subject\": subject,\\n        \"body\": body,\\n        \"message\": \"Approve sending this email?\",\\n    })\\n\\n    if response.get(\"action\") == \"approve\":\\n        final_to = response.get(\"to\", to)\\n        final_subject = response.get(\"subject\", subject)\\n        final_body = response.get(\"body\", body)\\n\\n        # Actually send the email (your implementation here)\\n        print(f\"[send_email] to={final_to} subject={final_subject} body={final_body}\")\\n        return f\"Email sent to {final_to}\"\\n\\n    return \"Email cancelled by user\"\\n\\n\\nmodel = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\").bind_tools([send_email])\\n\\n\\ndef agent_node(state: AgentState):\\n    # LLM may decide to call the tool; interrupt pauses before sending\\n    result = model.invoke(state[\"messages\"])\\n    return {\"messages\": state[\"messages\"] + [result]}\\n\\n\\nbuilder = StateGraph(AgentState)\\nbuilder.add_node(\"agent\", agent_node)\\nbuilder.add_edge(START, \"agent\")\\nbuilder.add_edge(\"agent\", END)\\n\\ncheckpointer = SqliteSaver(sqlite3.connect(\"tool-approval.db\"))\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"email-workflow\"}}\\ninitial = graph.invoke(\\n    {\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"Send an email to [email\\xa0protected] about the meeting\"}\\n        ]\\n    },\\n    config=config,\\n)\\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={\\'action\\': \\'send_email\\', ...})]\\n\\n# Resume with approval and optionally edited arguments\\nresumed = graph.invoke(\\n    Command(resume={\"action\": \"approve\", \"subject\": \"Updated subject\"}),\\n    config=config,\\n)\\nprint(resumed[\"messages\"][-1])  # -> Tool result returned by send_email\\n\\n\\u200bValidating human input\\nSometimes you need to validate input from humans and ask again if it’s invalid. You can do this using multiple interrupt calls in a loop.\\nCopyfrom langgraph.types import interrupt\\n\\ndef get_age_node(state: State):\\n    prompt = \"What is your age?\"\\n\\n    while True:\\n        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\\n\\n        # Validate the input\\n        if isinstance(answer, int) and answer > 0:\\n            # Valid input - continue\\n            break\\n        else:\\n            # Invalid input - ask again with a more specific prompt\\n            prompt = f\"\\'{answer}\\' is not a valid age. Please enter a positive number.\"\\n\\n    return {\"age\": answer}\\n\\nEach time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues.\\nFull exampleCopyimport sqlite3\\nfrom typing import TypedDict\\n\\nfrom langgraph.checkpoint.sqlite import SqliteSaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Command, interrupt\\n\\n\\nclass FormState(TypedDict):\\n    age: int | None\\n\\n\\ndef get_age_node(state: FormState):\\n    prompt = \"What is your age?\"\\n\\n    while True:\\n        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\\n\\n        if isinstance(answer, int) and answer > 0:\\n            return {\"age\": answer}\\n\\n        prompt = f\"\\'{answer}\\' is not a valid age. Please enter a positive number.\"\\n\\n\\nbuilder = StateGraph(FormState)\\nbuilder.add_node(\"collect_age\", get_age_node)\\nbuilder.add_edge(START, \"collect_age\")\\nbuilder.add_edge(\"collect_age\", END)\\n\\ncheckpointer = SqliteSaver(sqlite3.connect(\"forms.db\"))\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"form-1\"}}\\nfirst = graph.invoke({\"age\": None}, config=config)\\nprint(first[\"__interrupt__\"])  # -> [Interrupt(value=\\'What is your age?\\', ...)]\\n\\n# Provide invalid data; the node re-prompts\\nretry = graph.invoke(Command(resume=\"thirty\"), config=config)\\nprint(retry[\"__interrupt__\"])  # -> [Interrupt(value=\"\\'thirty\\' is not a valid age...\", ...)]\\n\\n# Provide valid data; loop exits and state updates\\nfinal = graph.invoke(Command(resume=30), config=config)\\nprint(final[\"age\"])  # -> 30\\n\\n\\u200bRules of interrupts\\nWhen you call interrupt within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input.\\nWhen execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning—it does not resume from the exact line where interrupt was called. This means any code that ran before the interrupt will execute again. Because of this, there’s a few important rules to follow when working with interrupts to ensure they behave as expected.\\n\\u200bDo not wrap interrupt calls in try/except\\nThe way that interrupt pauses execution at the point of the call is by throwing a special exception. If you wrap the interrupt call in a try/except block, you will catch this exception and the interrupt will not be passed back to the graph.\\n\\n✅ Separate interrupt calls from error-prone code\\n✅ Use specific exception types in try/except blocks\\n\\nSeparating logicExplicit exception handlingCopydef node_a(state: State):\\n    # ✅ Good: interrupting first, then handling\\n    # error conditions separately\\n    interrupt(\"What\\'s your name?\")\\n    try:\\n        fetch_data()  # This can fail\\n    except Exception as e:\\n        print(e)\\n    return state\\n\\n\\n🔴 Do not wrap interrupt calls in bare try/except blocks\\n\\nCopydef node_a(state: State):\\n    # ❌ Bad: wrapping interrupt in bare try/except\\n    # will catch the interrupt exception\\n    try:\\n        interrupt(\"What\\'s your name?\")\\n    except Exception as e:\\n        print(e)\\n    return state\\n\\n\\u200bDo not reorder interrupt calls within a node\\nIt’s common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully.\\nWhen a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task’s resume list. Matching is strictly index-based, so the order of interrupt calls within the node is important.\\n\\n✅ Keep interrupt calls consistent across node executions\\n\\nCopydef node_a(state: State):\\n    # ✅ Good: interrupt calls happen in the same order every time\\n    name = interrupt(\"What\\'s your name?\")\\n    age = interrupt(\"What\\'s your age?\")\\n    city = interrupt(\"What\\'s your city?\")\\n\\n    return {\\n        \"name\": name,\\n        \"age\": age,\\n        \"city\": city\\n    }\\n\\n\\n🔴 Do not conditionally skip interrupt calls within a node\\n🔴 Do not loop interrupt calls using logic that isn’t deterministic across executions\\n\\nSkipping interruptsLooping interruptsCopydef node_a(state: State):\\n    # ❌ Bad: conditionally skipping interrupts changes the order\\n    name = interrupt(\"What\\'s your name?\")\\n\\n    # On first run, this might skip the interrupt\\n    # On resume, it might not skip it - causing index mismatch\\n    if state.get(\"needs_age\"):\\n        age = interrupt(\"What\\'s your age?\")\\n\\n    city = interrupt(\"What\\'s your city?\")\\n\\n    return {\"name\": name, \"city\": city}\\n\\n\\u200bDo not return complex values in interrupt calls\\nDepending on which checkpointer is used, complex values may not be serializable (e.g. you can’t serialize a function). To make your graphs adaptable to any deployment, it’s best practice to only use values that can be reasonably serialized.\\n\\n✅ Pass simple, JSON-serializable types to interrupt\\n✅ Pass dictionaries/objects with simple values\\n\\nSimple valuesStructured dataCopydef node_a(state: State):\\n    # ✅ Good: passing simple types that are serializable\\n    name = interrupt(\"What\\'s your name?\")\\n    count = interrupt(42)\\n    approved = interrupt(True)\\n\\n    return {\"name\": name, \"count\": count, \"approved\": approved}\\n\\n\\n🔴 Do not pass functions, class instances, or other complex objects to interrupt\\n\\nFunctionsClass instancesCopydef validate_input(value):\\n    return len(value) > 0\\n\\ndef node_a(state: State):\\n    # ❌ Bad: passing a function to interrupt\\n    # The function cannot be serialized\\n    response = interrupt({\\n        \"question\": \"What\\'s your name?\",\\n        \"validator\": validate_input  # This will fail\\n    })\\n    return {\"name\": response}\\n\\n\\u200bSide effects called before interrupt must be idempotent\\nBecause interrupts work by re-running the nodes they were called from, side effects called before interrupt should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution.\\nAs an example, you might have an API call to update a record inside of a node. If interrupt is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records.\\n\\n✅ Use idempotent operations before interrupt\\n✅ Place side effects after interrupt calls\\n✅ Separate side effects into separate nodes when possible\\n\\nIdempotent operationsSide effects after interruptSeparating into different nodesCopydef node_a(state: State):\\n    # ✅ Good: using upsert operation which is idempotent\\n    # Running this multiple times will have the same result\\n    db.upsert_user(\\n        user_id=state[\"user_id\"],\\n        status=\"pending_approval\"\\n    )\\n\\n    approved = interrupt(\"Approve this change?\")\\n\\n    return {\"approved\": approved}\\n\\n\\n🔴 Do not perform non-idempotent operations before interrupt\\n🔴 Do not create new records without checking if they exist\\n\\nCreating recordsAppending to listsCopydef node_a(state: State):\\n    # ❌ Bad: creating a new record before interrupt\\n    # This will create duplicate records on each resume\\n    audit_id = db.create_audit_log({\\n        \"user_id\": state[\"user_id\"],\\n        \"action\": \"pending_approval\",\\n        \"timestamp\": datetime.now()\\n    })\\n\\n    approved = interrupt(\"Approve this change?\")\\n\\n    return {\"approved\": approved, \"audit_id\": audit_id}\\n\\n\\u200bUsing with subgraphs called as functions\\nWhen invoking a subgraph within a node, the parent graph will resume execution from the beginning of the node where the subgraph was invoked and the interrupt was triggered. Similarly, the subgraph will also resume from the beginning of the node where interrupt was called.\\nCopydef node_in_parent_graph(state: State):\\n    some_code()  # <-- This will re-execute when resumed\\n    # Invoke a subgraph as a function.\\n    # The subgraph contains an `interrupt` call.\\n    subgraph_result = subgraph.invoke(some_input)\\n    # ...\\n\\ndef node_in_subgraph(state: State):\\n    some_other_code()  # <-- This will also re-execute when resumed\\n    result = interrupt(\"What\\'s your name?\")\\n    # ...\\n\\n\\u200bDebugging with interrupts\\nTo debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying interrupt_before and interrupt_after when compiling the graph.\\nStatic interrupts are not recommended for human-in-the-loop workflows. Use the interrupt function instead.\\n At compile time At run timeCopygraph = builder.compile(\\n    interrupt_before=[\"node_a\"],  \\n    interrupt_after=[\"node_b\", \"node_c\"],  \\n    checkpointer=checkpointer,\\n)\\n\\n# Pass a thread ID to the graph\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread\"\\n    }\\n}\\n\\n# Run the graph until the breakpoint\\ngraph.invoke(inputs, config=config)  \\n\\n# Resume the graph\\ngraph.invoke(None, config=config)  \\n\\nThe breakpoints are set during compile time.\\ninterrupt_before specifies the nodes where execution should pause before the node is executed.\\ninterrupt_after specifies the nodes where execution should pause after the node is executed.\\nA checkpointer is required to enable breakpoints.\\nThe graph is run until the first breakpoint is hit.\\nThe graph is resumed by passing in None for the input. This will run the graph until the next breakpoint is hit.\\nCopyconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread\"\\n    }\\n}\\n\\n# Run the graph until the breakpoint\\ngraph.invoke(\\n    inputs,\\n    interrupt_before=[\"node_a\"],  \\n    interrupt_after=[\"node_b\", \"node_c\"],  \\n    config=config,\\n)\\n\\n# Resume the graph\\ngraph.invoke(None, config=config)  \\n\\ngraph.invoke is called with the interrupt_before and interrupt_after parameters. This is a run-time configuration and can be changed for every invocation.\\ninterrupt_before specifies the nodes where execution should pause before the node is executed.\\ninterrupt_after specifies the nodes where execution should pause after the node is executed.\\nThe graph is run until the first breakpoint is hit.\\nThe graph is resumed by passing in None for the input. This will run the graph until the next breakpoint is hit.\\n\\n\\u200bUsing LangGraph Studio\\nYou can use LangGraph Studio to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoStreamingPreviousUse time-travelNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=['https://docs.langchain.com/oss/python/langgraph/overview',\n",
    "      'https://docs.langchain.com/oss/python/langgraph/workflows-agents',\n",
    "      'https://docs.langchain.com/oss/python/langgraph/interrupts']\n",
    "\n",
    "\n",
    "docs = WebBaseLoader(web_paths=urls).load()\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "641cb807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 67\n"
     ]
    }
   ],
   "source": [
    "chunks = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100).split_documents(docs)\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c0b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce2cafce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='4e734325-4068-48fe-a7a9-c0ef4793839b', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='called before interrupt must be idempotentUsing with subgraphs called as functionsDebugging with interruptsUsing LangGraph StudioCapabilitiesInterruptsCopy pageCopy pageInterrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its persistence layer and waits indefinitely until you resume execution.'),\n",
       " Document(id='be473abf-e7b6-423b-b934-d7c675b6bb53', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bRules of interrupts\\nWhen you call interrupt within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input.\\nWhen execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning—it does not resume from the exact line where interrupt was called. This means any code that ran before the interrupt will execute again. Because of this, there’s a few important rules to follow when working with interrupts to ensure they behave as expected.\\n\\u200bDo not wrap interrupt calls in try/except\\nThe way that interrupt pauses execution at the point of the call is by throwing a special exception. If you wrap the interrupt call in a try/except block, you will catch this exception and the interrupt will not be passed back to the graph.'),\n",
       " Document(id='6298e293-4900-4bfc-a4af-6140188433b0', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='Interrupts - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesInterruptsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pagePause using interruptResuming interruptsCommon patternsApprove or rejectReview and edit stateInterrupts in toolsValidating human inputRules of interruptsDo not wrap interrupt calls in try/exceptDo not reorder interrupt calls within a nodeDo not return complex values in interrupt callsSide effects called before interrupt must be idempotentUsing with subgraphs called as functionsDebugging with'),\n",
       " Document(id='1da9bf9e-461c-4b82-bf05-6d4901a21d7b', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bCommon patterns\\nThe key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:\\n\\n Approval workflows: Pause before executing critical actions (API calls, database changes, financial transactions)\\n Review and edit: Let humans review and modify LLM outputs or tool calls before continuing\\n Interrupting tool calls: Pause before executing tool calls to review and edit the tool call before execution\\n Validating human input: Pause before proceeding to the next step to validate human input\\n\\n\\u200bApprove or reject\\nOne of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.\\nCopyfrom typing import Literal\\nfrom langgraph.types import interrupt, Command')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Give me some rules of interrupt in langgraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd0e5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating retriever tool\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "langgraph_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"langgraph retriever\",\n",
    "    description=\"this tool retrieves relevant documents about langgraph based on query\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82947cb0",
   "metadata": {},
   "source": [
    "#### Langchain documents retriever tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e75ad54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='Agents - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsAgentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageCore componentsModelStatic modelDynamic modelToolsDefining toolsTool error handlingTool use in the ReAct loopSystem promptDynamic system promptInvocationAdvanced conceptsStructured outputToolStrategyProviderStrategyMemoryDefining state via middlewareDefining state via state_schemaStreamingMiddlewareCore componentsAgentsCopy pageCopy pageAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\\ncreate_agent provides a production-ready agent implementation.\\nAn LLM Agent runs tools in a loop to achieve a goal.\\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\\n\\ncreate_agent builds a graph-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.Learn more about the Graph API.\\n\\u200bCore components\\n\\u200bModel\\nThe model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\\n\\u200bStatic model\\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\\nTo initialize a static model from a model identifier string:\\nCopyfrom langchain.agents import create_agent\\n\\nagent = create_agent(\"gpt-5\", tools=tools)\\n\\nModel identifier strings support automatic inference (e.g., \"gpt-5\" will be inferred as \"openai:gpt-5\"). Refer to the reference to see a full list of model identifier string mappings.\\nFor more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use ChatOpenAI. See Chat models for other available chat model classes.\\nCopyfrom langchain.agents import create_agent\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(\\n    model=\"gpt-5\",\\n    temperature=0.1,\\n    max_tokens=1000,\\n    timeout=30\\n    # ... (other params)\\n)\\nagent = create_agent(model, tools=tools)\\n\\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like temperature, max_tokens, timeouts, base_url, and other provider-specific settings. Refer to the reference to see available params and methods on your model.\\n\\u200bDynamic model\\nDynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.\\nTo use a dynamic model, create middleware using the @wrap_model_call decorator that modifies the model in the request:\\nCopyfrom langchain_openai import ChatOpenAI\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\\n\\n\\nbasic_model = ChatOpenAI(model=\"gpt-4o-mini\")\\nadvanced_model = ChatOpenAI(model=\"gpt-4o\")\\n\\n@wrap_model_call\\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\\n    \"\"\"Choose model based on conversation complexity.\"\"\"\\n    message_count = len(request.state[\"messages\"])\\n\\n    if message_count > 10:\\n        # Use an advanced model for longer conversations\\n        model = advanced_model\\n    else:\\n        model = basic_model\\n\\n    return handler(request.override(model=model))\\n\\nagent = create_agent(\\n    model=basic_model,  # Default model\\n    tools=tools,\\n    middleware=[dynamic_model_selection]\\n)\\n\\nPre-bound models (models with bind_tools already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.\\nFor model configuration details, see Models. For dynamic model selection patterns, see Dynamic model in middleware.\\n\\u200bTools\\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\\n\\nMultiple tool calls in sequence (triggered by a single prompt)\\nParallel tool calls when appropriate\\nDynamic tool selection based on previous results\\nTool retry logic and error handling\\nState persistence across tool calls\\n\\nFor more information, see Tools.\\n\\u200bDefining tools\\nPass a list of tools to the agent.\\nTools can be specified as plain Python functions or coroutines.The tool decorator can be used to customize tool names, descriptions, argument schemas, and other properties.\\nCopyfrom langchain.tools import tool\\nfrom langchain.agents import create_agent\\n\\n\\n@tool\\ndef search(query: str) -> str:\\n    \"\"\"Search for information.\"\"\"\\n    return f\"Results for: {query}\"\\n\\n@tool\\ndef get_weather(location: str) -> str:\\n    \"\"\"Get weather information for a location.\"\"\"\\n    return f\"Weather in {location}: Sunny, 72°F\"\\n\\nagent = create_agent(model, tools=[search, get_weather])\\n\\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\\n\\u200bTool error handling\\nTo customize how tool errors are handled, use the @wrap_tool_call decorator to create middleware:\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import wrap_tool_call\\nfrom langchain.messages import ToolMessage\\n\\n\\n@wrap_tool_call\\ndef handle_tool_errors(request, handler):\\n    \"\"\"Handle tool execution errors with custom messages.\"\"\"\\n    try:\\n        return handler(request)\\n    except Exception as e:\\n        # Return a custom error message to the model\\n        return ToolMessage(\\n            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\\n            tool_call_id=request.tool_call[\"id\"]\\n        )\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[search, get_weather],\\n    middleware=[handle_tool_errors]\\n)\\n\\nThe agent will return a ToolMessage with the custom error message when a tool fails:\\nCopy[\\n    ...\\n    ToolMessage(\\n        content=\"Tool error: Please check your input and try again. (division by zero)\",\\n        tool_call_id=\"...\"\\n    ),\\n    ...\\n]\\n\\n\\u200bTool use in the ReAct loop\\nAgents follow the ReAct (“Reasoning + Acting”) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\\nExample of ReAct loopPrompt: Identify the current most popular wireless headphones and verify availability.Copy================================ Human Message =================================\\n\\nFind the most popular wireless headphones right now and check if they\\'re in stock\\n\\nReasoning: “Popularity is time-sensitive, I need to use the provided search tool.”\\nActing: Call search_products(\"wireless headphones\")\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  search_products (call_abc123)\\n Call ID: call_abc123\\n  Args:\\n    query: wireless headphones\\nCopy================================= Tool Message =================================\\n\\nFound 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\\n\\nReasoning: “I need to confirm availability for the top-ranked item before answering.”\\nActing: Call check_inventory(\"WH-1000XM5\")\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  check_inventory (call_def456)\\n Call ID: call_def456\\n  Args:\\n    product_id: WH-1000XM5\\nCopy================================= Tool Message =================================\\n\\nProduct WH-1000XM5: 10 units in stock\\n\\nReasoning: “I have the most popular model and its stock status. I can now answer the user’s question.”\\nActing: Produce final answer\\nCopy================================== Ai Message ==================================\\n\\nI found wireless headphones (model WH-1000XM5) with 10 units in stock...\\n\\nTo learn more about tools, see Tools.\\n\\u200bSystem prompt\\nYou can shape how your agent approaches tasks by providing a prompt. The system_prompt parameter can be provided as a string:\\nCopyagent = create_agent(\\n    model,\\n    tools,\\n    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\\n)\\n\\nWhen no system_prompt is provided, the agent will infer its task from the messages directly.\\nThe system_prompt parameter accepts either a str or a SystemMessage. Using a SystemMessage gives you more control over the prompt structure, which is useful for provider-specific features like Anthropic’s prompt caching:\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.messages import SystemMessage, HumanMessage\\n\\nliterary_agent = create_agent(\\n    model=\"anthropic:claude-sonnet-4-5\",\\n    system_prompt=SystemMessage(\\n        content=[\\n            {\\n                \"type\": \"text\",\\n                \"text\": \"You are an AI assistant tasked with analyzing literary works.\",\\n            },\\n            {\\n                \"type\": \"text\",\\n                \"text\": \"<the entire contents of \\'Pride and Prejudice\\'>\",\\n                \"cache_control\": {\"type\": \"ephemeral\"}\\n            }\\n        ]\\n    )\\n)\\n\\nresult = literary_agent.invoke(\\n    {\"messages\": [HumanMessage(\"Analyze the major themes in \\'Pride and Prejudice\\'.\")]}\\n)\\n\\nThe cache_control field with {\"type\": \"ephemeral\"} tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\\n\\u200bDynamic system prompt\\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware.\\nThe @dynamic_prompt decorator creates middleware that generates system prompts based on the model request:\\nCopyfrom typing import TypedDict\\n\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n\\nclass Context(TypedDict):\\n    user_role: str\\n\\n@dynamic_prompt\\ndef user_role_prompt(request: ModelRequest) -> str:\\n    \"\"\"Generate system prompt based on user role.\"\"\"\\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\\n    base_prompt = \"You are a helpful assistant.\"\\n\\n    if user_role == \"expert\":\\n        return f\"{base_prompt} Provide detailed technical responses.\"\\n    elif user_role == \"beginner\":\\n        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\\n\\n    return base_prompt\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[web_search],\\n    middleware=[user_role_prompt],\\n    context_schema=Context\\n)\\n\\n# The system prompt will be set dynamically based on context\\nresult = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\\n    context={\"user_role\": \"expert\"}\\n)\\n\\nFor more details on message types and formatting, see Messages. For comprehensive middleware documentation, see Middleware.\\n\\u200bInvocation\\nYou can invoke an agent by passing an update to its State. All agents include a sequence of messages in their state; to invoke the agent, pass a new message:\\nCopyresult = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What\\'s the weather in San Francisco?\"}]}\\n)\\n\\nFor streaming steps and / or tokens from the agent, refer to the streaming guide.\\nOtherwise, the agent follows the LangGraph Graph API and supports all associated methods, such as stream and invoke.\\n\\u200bAdvanced concepts\\n\\u200bStructured output\\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the response_format parameter.\\n\\u200bToolStrategy\\nToolStrategy uses artificial tool calling to generate structured output. This works with any model that supports tool calling:\\nCopyfrom pydantic import BaseModel\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.structured_output import ToolStrategy\\n\\n\\nclass ContactInfo(BaseModel):\\n    name: str\\n    email: str\\n    phone: str\\n\\nagent = create_agent(\\n    model=\"gpt-4o-mini\",\\n    tools=[search_tool],\\n    response_format=ToolStrategy(ContactInfo)\\n)\\n\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, [email\\xa0protected], (555) 123-4567\"}]\\n})\\n\\nresult[\"structured_response\"]\\n# ContactInfo(name=\\'John Doe\\', email=\\'[email\\xa0protected]\\', phone=\\'(555) 123-4567\\')\\n\\n\\u200bProviderStrategy\\nProviderStrategy uses the model provider’s native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):\\nCopyfrom langchain.agents.structured_output import ProviderStrategy\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    response_format=ProviderStrategy(ContactInfo)\\n)\\n\\nAs of langchain 1.0, simply passing a schema (e.g., response_format=ContactInfo) is no longer supported. You must explicitly use ToolStrategy or ProviderStrategy.\\nTo learn about structured output, see Structured output.\\n\\u200bMemory\\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\\nInformation stored in the state can be thought of as the short-term memory of the agent:\\nCustom state schemas must extend AgentState as a TypedDict.\\nThere are two ways to define custom state:\\n\\nVia middleware (preferred)\\nVia state_schema on create_agent\\n\\n\\u200bDefining state via middleware\\nUse middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.\\nCopyfrom langchain.agents import AgentState\\nfrom langchain.agents.middleware import AgentMiddleware\\nfrom typing import Any\\n\\n\\nclass CustomState(AgentState):\\n    user_preferences: dict\\n\\nclass CustomMiddleware(AgentMiddleware):\\n    state_schema = CustomState\\n    tools = [tool1, tool2]\\n\\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\\n        ...\\n\\nagent = create_agent(\\n    model,\\n    tools=tools,\\n    middleware=[CustomMiddleware()]\\n)\\n\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\\n})\\n\\n\\u200bDefining state via state_schema\\nUse the state_schema parameter as a shortcut to define custom state that is only used in tools.\\nCopyfrom langchain.agents import AgentState\\n\\n\\nclass CustomState(AgentState):\\n    user_preferences: dict\\n\\nagent = create_agent(\\n    model,\\n    tools=[tool1, tool2],\\n    state_schema=CustomState\\n)\\n# The agent can now track additional state beyond messages\\nresult = agent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\\n})\\n\\nAs of langchain 1.0, custom state schemas must be TypedDict types. Pydantic models and dataclasses are no longer supported. See the v1 migration guide for more details.\\nDefining custom state via middleware is preferred over defining it via state_schema on create_agent because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.state_schema is still supported for backwards compatibility on create_agent.\\nTo learn more about memory, see Memory. For information on implementing long-term memory that persists across sessions, see Long-term memory.\\n\\u200bStreaming\\nWe’ve seen how the agent can be called with invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\\nCopyfor chunk in agent.stream({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\\n}, stream_mode=\"values\"):\\n    # Each chunk contains the full state at that point\\n    latest_message = chunk[\"messages\"][-1]\\n    if latest_message.content:\\n        print(f\"Agent: {latest_message.content}\")\\n    elif latest_message.tool_calls:\\n        print(f\"Calling tools: {[tc[\\'name\\'] for tc in latest_message.tool_calls]}\")\\n\\nFor more details on streaming, see Streaming.\\n\\u200bMiddleware\\nMiddleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\\n\\nProcess state before the model is called (e.g., message trimming, context injection)\\nModify or validate the model’s response (e.g., guardrails, content filtering)\\nHandle tool execution errors with custom logic\\nImplement dynamic model selection based on state or context\\nAdd custom logging, monitoring, or analytics\\n\\nMiddleware integrates seamlessly into the agent’s execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\\nFor comprehensive middleware documentation including decorators like @before_model, @after_model, and @wrap_tool_call, see Middleware.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoPhilosophyPreviousModelsNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/tools', 'title': 'Tools - Docs by LangChain', 'language': 'en'}, page_content='Tools - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsToolsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageCreate toolsBasic tool definitionCustomize tool propertiesCustom tool nameCustom tool descriptionAdvanced schema definitionReserved argument namesAccessing ContextToolRuntimeContextMemory (Store)Stream WriterCore componentsToolsCopy pageCopy pageTools extend what agents can do—letting them fetch real-time data, execute code, query external databases, and take actions in the world.\\nUnder the hood, tools are callable functions with well-defined inputs and outputs that get passed to a chat model. The model decides when to invoke a tool based on the conversation context, and what input arguments to provide.\\nFor details on how models handle tool calls, see Tool calling.\\n\\u200bCreate tools\\n\\u200bBasic tool definition\\nThe simplest way to create a tool is with the @tool decorator. By default, the function’s docstring becomes the tool’s description that helps the model understand when to use it:\\nCopyfrom langchain.tools import tool\\n\\n@tool\\ndef search_database(query: str, limit: int = 10) -> str:\\n    \"\"\"Search the customer database for records matching the query.\\n\\n    Args:\\n        query: Search terms to look for\\n        limit: Maximum number of results to return\\n    \"\"\"\\n    return f\"Found {limit} results for \\'{query}\\'\"\\n\\nType hints are required as they define the tool’s input schema. The docstring should be informative and concise to help the model understand the tool’s purpose.\\nServer-side tool useSome chat models (e.g., OpenAI, Anthropic, and Gemini) feature built-in tools that are executed server-side, such as web search and code interpreters. Refer to the provider overview to learn how to access these tools with your specific chat model.\\n\\u200bCustomize tool properties\\n\\u200bCustom tool name\\nBy default, the tool name comes from the function name. Override it when you need something more descriptive:\\nCopy@tool(\"web_search\")  # Custom name\\ndef search(query: str) -> str:\\n    \"\"\"Search the web for information.\"\"\"\\n    return f\"Results for: {query}\"\\n\\nprint(search.name)  # web_search\\n\\n\\u200bCustom tool description\\nOverride the auto-generated tool description for clearer model guidance:\\nCopy@tool(\"calculator\", description=\"Performs arithmetic calculations. Use this for any math problems.\")\\ndef calc(expression: str) -> str:\\n    \"\"\"Evaluate mathematical expressions.\"\"\"\\n    return str(eval(expression))\\n\\n\\u200bAdvanced schema definition\\nDefine complex inputs with Pydantic models or JSON schemas:\\nPydantic modelJSON SchemaCopyfrom pydantic import BaseModel, Field\\nfrom typing import Literal\\n\\nclass WeatherInput(BaseModel):\\n    \"\"\"Input for weather queries.\"\"\"\\n    location: str = Field(description=\"City name or coordinates\")\\n    units: Literal[\"celsius\", \"fahrenheit\"] = Field(\\n        default=\"celsius\",\\n        description=\"Temperature unit preference\"\\n    )\\n    include_forecast: bool = Field(\\n        default=False,\\n        description=\"Include 5-day forecast\"\\n    )\\n\\n@tool(args_schema=WeatherInput)\\ndef get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\\n    \"\"\"Get current weather and optional forecast.\"\"\"\\n    temp = 22 if units == \"celsius\" else 72\\n    result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\\n    if include_forecast:\\n        result += \"\\\\nNext 5 days: Sunny\"\\n    return result\\n\\n\\u200bReserved argument names\\nThe following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors.\\nParameter namePurposeconfigReserved for passing RunnableConfig to tools internallyruntimeReserved for ToolRuntime parameter (accessing state, context, store)\\nTo access runtime information, use the ToolRuntime parameter instead of naming your own arguments config or runtime.\\n\\u200bAccessing Context\\nWhy this matters: Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.Runtime context provides a way to inject dependencies (like database connections, user IDs, or configuration) into your tools at runtime, making them more testable and reusable.\\nTools can access runtime information through the ToolRuntime parameter, which provides:\\n\\nState - Mutable data that flows through execution (e.g., messages, counters, custom fields)\\nContext - Immutable configuration like user IDs, session details, or application-specific configuration\\nStore - Persistent long-term memory across conversations\\nStream Writer - Stream custom updates as tools execute\\nConfig - RunnableConfig for the execution\\nTool Call ID - ID of the current tool call\\n\\n\\n\\u200bToolRuntime\\nUse ToolRuntime to access all runtime information in a single parameter. Simply add runtime: ToolRuntime to your tool signature, and it will be automatically injected without being exposed to the LLM.\\nToolRuntime: A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate InjectedState, InjectedStore, get_runtime, and InjectedToolCallId annotations.The runtime automatically provides these capabilities to your tool functions without you having to pass them explicitly or use global state.\\nAccessing state:\\nTools can access the current graph state using ToolRuntime:\\nCopyfrom langchain.tools import tool, ToolRuntime\\n\\n# Access the current conversation state\\n@tool\\ndef summarize_conversation(\\n    runtime: ToolRuntime\\n) -> str:\\n    \"\"\"Summarize the conversation so far.\"\"\"\\n    messages = runtime.state[\"messages\"]\\n\\n    human_msgs = sum(1 for m in messages if m.__class__.__name__ == \"HumanMessage\")\\n    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == \"AIMessage\")\\n    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == \"ToolMessage\")\\n\\n    return f\"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results\"\\n\\n# Access custom state fields\\n@tool\\ndef get_user_preference(\\n    pref_name: str,\\n    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model\\n) -> str:\\n    \"\"\"Get a user preference value.\"\"\"\\n    preferences = runtime.state.get(\"user_preferences\", {})\\n    return preferences.get(pref_name, \"Not set\")\\n\\nThe runtime parameter is hidden from the model. For the example above, the model only sees pref_name in the tool schema - runtime is not included in the request.\\nUpdating state:\\nUse Command to update the agent’s state or control the graph’s execution flow:\\nCopyfrom langgraph.types import Command\\nfrom langchain.messages import RemoveMessage\\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\\nfrom langchain.tools import tool, ToolRuntime\\n\\n# Update the conversation history by removing all messages\\n@tool\\ndef clear_conversation() -> Command:\\n    \"\"\"Clear the conversation history.\"\"\"\\n\\n    return Command(\\n        update={\\n            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],\\n        }\\n    )\\n\\n# Update the user_name in the agent state\\n@tool\\ndef update_user_name(\\n    new_name: str,\\n    runtime: ToolRuntime\\n) -> Command:\\n    \"\"\"Update the user\\'s name.\"\"\"\\n    return Command(update={\"user_name\": new_name})\\n\\n\\u200bContext\\nAccess immutable configuration and contextual data like user IDs, session details, or application-specific configuration through runtime.context.\\nTools can access runtime context through ToolRuntime:\\nCopyfrom dataclasses import dataclass\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.agents import create_agent\\nfrom langchain.tools import tool, ToolRuntime\\n\\n\\nUSER_DATABASE = {\\n    \"user123\": {\\n        \"name\": \"Alice Johnson\",\\n        \"account_type\": \"Premium\",\\n        \"balance\": 5000,\\n        \"email\": \"[email\\xa0protected]\"\\n    },\\n    \"user456\": {\\n        \"name\": \"Bob Smith\",\\n        \"account_type\": \"Standard\",\\n        \"balance\": 1200,\\n        \"email\": \"[email\\xa0protected]\"\\n    }\\n}\\n\\n@dataclass\\nclass UserContext:\\n    user_id: str\\n\\n@tool\\ndef get_account_info(runtime: ToolRuntime[UserContext]) -> str:\\n    \"\"\"Get the current user\\'s account information.\"\"\"\\n    user_id = runtime.context.user_id\\n\\n    if user_id in USER_DATABASE:\\n        user = USER_DATABASE[user_id]\\n        return f\"Account holder: {user[\\'name\\']}\\\\nType: {user[\\'account_type\\']}\\\\nBalance: ${user[\\'balance\\']}\"\\n    return \"User not found\"\\n\\nmodel = ChatOpenAI(model=\"gpt-4o\")\\nagent = create_agent(\\n    model,\\n    tools=[get_account_info],\\n    context_schema=UserContext,\\n    system_prompt=\"You are a financial assistant.\"\\n)\\n\\nresult = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What\\'s my current balance?\"}]},\\n    context=UserContext(user_id=\"user123\")\\n)\\n\\n\\u200bMemory (Store)\\nAccess persistent data across conversations using the store. The store is accessed via runtime.store and allows you to save and retrieve user-specific or application-specific data.\\nTools can access and update the store through ToolRuntime:\\nCopyfrom typing import Any\\nfrom langgraph.store.memory import InMemoryStore\\nfrom langchain.agents import create_agent\\nfrom langchain.tools import tool, ToolRuntime\\n\\n\\n# Access memory\\n@tool\\ndef get_user_info(user_id: str, runtime: ToolRuntime) -> str:\\n    \"\"\"Look up user info.\"\"\"\\n    store = runtime.store\\n    user_info = store.get((\"users\",), user_id)\\n    return str(user_info.value) if user_info else \"Unknown user\"\\n\\n# Update memory\\n@tool\\ndef save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:\\n    \"\"\"Save user info.\"\"\"\\n    store = runtime.store\\n    store.put((\"users\",), user_id, user_info)\\n    return \"Successfully saved user info.\"\\n\\nstore = InMemoryStore()\\nagent = create_agent(\\n    model,\\n    tools=[get_user_info, save_user_info],\\n    store=store\\n)\\n\\n# First session: save user info\\nagent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Save the following user: userid: abc123, name: Foo, age: 25, email: [email\\xa0protected]\"}]\\n})\\n\\n# Second session: get user info\\nagent.invoke({\\n    \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id \\'abc123\\'\"}]\\n})\\n# Here is the user info for user with ID \"abc123\":\\n# - Name: Foo\\n# - Age: 25\\n# - Email: [email\\xa0protected]\\nSee all 42 lines\\n\\u200bStream Writer\\nStream custom updates from tools as they execute using runtime.stream_writer. This is useful for providing real-time feedback to users about what a tool is doing.\\nCopyfrom langchain.tools import tool, ToolRuntime\\n\\n@tool\\ndef get_weather(city: str, runtime: ToolRuntime) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    writer = runtime.stream_writer\\n\\n    # Stream custom updates as the tool executes\\n    writer(f\"Looking up data for city: {city}\")\\n    writer(f\"Acquired data for city: {city}\")\\n\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nIf you use runtime.stream_writer inside your tool, the tool must be invoked within a LangGraph execution context. See Streaming for more details.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoMessagesPreviousShort-term memoryNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='Streaming - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsStreamingLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageOverviewSupported stream modesAgent progressLLM tokensCustom updatesStream multiple modesCommon patternsStreaming tool callsAccessing completed messagesStreaming with human-in-the-loopStreaming from sub-agentsDisable streamingRelatedCore componentsStreamingCopy pageCopy pageLangChain implements a streaming system to surface real-time updates.\\nStreaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\\n\\u200bOverview\\nLangChain’s streaming system lets you surface live feedback from agent runs to your application.\\nWhat’s possible with LangChain streaming:\\n\\n Stream agent progress — get state updates after each agent step.\\n Stream LLM tokens — stream language model tokens as they’re generated.\\n Stream custom updates — emit user-defined signals (e.g., \"Fetched 10/100 records\").\\n Stream multiple modes — choose from updates (agent progress), messages (LLM tokens + metadata), or custom (arbitrary user data).\\n\\nSee the common patterns section below for additional end-to-end examples.\\n\\u200bSupported stream modes\\nPass one or more of the following stream modes as a list to the stream or astream methods:\\nModeDescriptionupdatesStreams state updates after each agent step. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.messagesStreams tuples of (token, metadata) from any graph nodes where an LLM is invoked.customStreams custom data from inside your graph nodes using the stream writer.\\n\\u200bAgent progress\\nTo stream agent progress, use the stream or astream methods with stream_mode=\"updates\". This emits an event after every agent step.\\nFor example, if you have an agent that calls a tool once, you should see the following updates:\\n\\nLLM node: AIMessage with tool call requests\\nTool node: ToolMessage with execution result\\nLLM node: Final AI response\\n\\nStreaming agent progressCopyfrom langchain.agents import create_agent\\n\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"gpt-5-nano\",\\n    tools=[get_weather],\\n)\\nfor chunk in agent.stream(  \\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\\n    stream_mode=\"updates\",\\n):\\n    for step, data in chunk.items():\\n        print(f\"step: {step}\")\\n        print(f\"content: {data[\\'messages\\'][-1].content_blocks}\")\\n\\nOutputCopystep: model\\ncontent: [{\\'type\\': \\'tool_call\\', \\'name\\': \\'get_weather\\', \\'args\\': {\\'city\\': \\'San Francisco\\'}, \\'id\\': \\'call_OW2NYNsNSKhRZpjW0wm2Aszd\\'}]\\n\\nstep: tools\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \"It\\'s always sunny in San Francisco!\"}]\\n\\nstep: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\'It\\'s always sunny in San Francisco!\\'}]\\n\\n\\u200bLLM tokens\\nTo stream tokens as they are produced by the LLM, use stream_mode=\"messages\". Below you can see the output of the agent streaming tool calls and the final response.\\nStreaming LLM tokensCopyfrom langchain.agents import create_agent\\n\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"gpt-5-nano\",\\n    tools=[get_weather],\\n)\\nfor token, metadata in agent.stream(  \\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\\n    stream_mode=\"messages\",\\n):\\n    print(f\"node: {metadata[\"langgraph_node\"]}\")\\n    print(f\"content: {token.content_blocks}\")\\n    print(\"\\\\n\")\\n\\nOutputCopynode: model\\ncontent: [{\\'type\\': \\'tool_call_chunk\\', \\'id\\': \\'call_vbCyBcP8VuneUzyYlSBZZsVa\\', \\'name\\': \\'get_weather\\', \\'args\\': \\'\\', \\'index\\': 0}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'tool_call_chunk\\', \\'id\\': None, \\'name\\': None, \\'args\\': \\'{\"\\', \\'index\\': 0}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'tool_call_chunk\\', \\'id\\': None, \\'name\\': None, \\'args\\': \\'city\\', \\'index\\': 0}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'tool_call_chunk\\', \\'id\\': None, \\'name\\': None, \\'args\\': \\'\":\"\\', \\'index\\': 0}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'tool_call_chunk\\', \\'id\\': None, \\'name\\': None, \\'args\\': \\'San\\', \\'index\\': 0}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'tool_call_chunk\\', \\'id\\': None, \\'name\\': None, \\'args\\': \\' Francisco\\', \\'index\\': 0}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'tool_call_chunk\\', \\'id\\': None, \\'name\\': None, \\'args\\': \\'\"}\\', \\'index\\': 0}]\\n\\n\\nnode: model\\ncontent: []\\n\\n\\nnode: tools\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \"It\\'s always sunny in San Francisco!\"}]\\n\\n\\nnode: model\\ncontent: []\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\'Here\\'}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\'\\'s\\'}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\' what\\'}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\' I\\'}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\' got\\'}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\':\\'}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\' \"\\'}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \"It\\'s\"}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\' always\\'}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\' sunny\\'}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\' in\\'}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\' San\\'}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\' Francisco\\'}]\\n\\n\\nnode: model\\ncontent: [{\\'type\\': \\'text\\', \\'text\\': \\'!\"\\\\n\\\\n\\'}]\\nSee all 94 lines\\n\\u200bCustom updates\\nTo stream updates from tools as they are executed, you can use get_stream_writer.\\nStreaming custom updatesCopyfrom langchain.agents import create_agent\\nfrom langgraph.config import get_stream_writer  \\n\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    writer = get_stream_writer()  \\n    # stream any arbitrary data\\n    writer(f\"Looking up data for city: {city}\")\\n    writer(f\"Acquired data for city: {city}\")\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[get_weather],\\n)\\n\\nfor chunk in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\\n    stream_mode=\"custom\"\\n):\\n    print(chunk)\\n\\nOutputCopyLooking up data for city: San Francisco\\nAcquired data for city: San Francisco\\n\\nIf you add get_stream_writer inside your tool, you won’t be able to invoke the tool outside of a LangGraph execution context.\\n\\u200bStream multiple modes\\nYou can specify multiple streaming modes by passing stream mode as a list: stream_mode=[\"updates\", \"custom\"].\\nThe streamed outputs will be tuples of (mode, chunk) where mode is the name of the stream mode and chunk is the data streamed by that mode.\\nStreaming multiple modesCopyfrom langchain.agents import create_agent\\nfrom langgraph.config import get_stream_writer\\n\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    writer = get_stream_writer()\\n    writer(f\"Looking up data for city: {city}\")\\n    writer(f\"Acquired data for city: {city}\")\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"gpt-5-nano\",\\n    tools=[get_weather],\\n)\\n\\nfor stream_mode, chunk in agent.stream(  \\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\\n    stream_mode=[\"updates\", \"custom\"]\\n):\\n    print(f\"stream_mode: {stream_mode}\")\\n    print(f\"content: {chunk}\")\\n    print(\"\\\\n\")\\n\\nOutputCopystream_mode: updates\\ncontent: {\\'model\\': {\\'messages\\': [AIMessage(content=\\'\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 280, \\'prompt_tokens\\': 132, \\'total_tokens\\': 412, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 256, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_provider\\': \\'openai\\', \\'model_name\\': \\'gpt-5-nano-2025-08-07\\', \\'system_fingerprint\\': None, \\'id\\': \\'chatcmpl-C9tlgBzGEbedGYxZ0rTCz5F7OXpL7\\', \\'service_tier\\': \\'default\\', \\'finish_reason\\': \\'tool_calls\\', \\'logprobs\\': None}, id=\\'lc_run--480c07cb-e405-4411-aa7f-0520fddeed66-0\\', tool_calls=[{\\'name\\': \\'get_weather\\', \\'args\\': {\\'city\\': \\'San Francisco\\'}, \\'id\\': \\'call_KTNQIftMrl9vgNwEfAJMVu7r\\', \\'type\\': \\'tool_call\\'}], usage_metadata={\\'input_tokens\\': 132, \\'output_tokens\\': 280, \\'total_tokens\\': 412, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 256}})]}}\\n\\n\\nstream_mode: custom\\ncontent: Looking up data for city: San Francisco\\n\\n\\nstream_mode: custom\\ncontent: Acquired data for city: San Francisco\\n\\n\\nstream_mode: updates\\ncontent: {\\'tools\\': {\\'messages\\': [ToolMessage(content=\"It\\'s always sunny in San Francisco!\", name=\\'get_weather\\', tool_call_id=\\'call_KTNQIftMrl9vgNwEfAJMVu7r\\')]}}\\n\\n\\nstream_mode: updates\\ncontent: {\\'model\\': {\\'messages\\': [AIMessage(content=\\'San Francisco weather: It\\'s always sunny in San Francisco!\\\\n\\\\n\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 764, \\'prompt_tokens\\': 168, \\'total_tokens\\': 932, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 704, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_provider\\': \\'openai\\', \\'model_name\\': \\'gpt-5-nano-2025-08-07\\', \\'system_fingerprint\\': None, \\'id\\': \\'chatcmpl-C9tljDFVki1e1haCyikBptAuXuHYG\\', \\'service_tier\\': \\'default\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'lc_run--acbc740a-18fe-4a14-8619-da92a0d0ee90-0\\', usage_metadata={\\'input_tokens\\': 168, \\'output_tokens\\': 764, \\'total_tokens\\': 932, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 704}})]}}\\n\\n\\u200bCommon patterns\\nBelow are examples showing common use cases for streaming.\\n\\u200bStreaming tool calls\\nYou may want to stream both:\\n\\nPartial JSON as tool calls are generated\\nThe completed, parsed tool calls that are executed\\n\\nSpecifying stream_mode=\"messages\" will stream incremental message chunks generated by all LLM calls in the agent. To access the completed messages with parsed tool calls:\\n\\nIf those messages are tracked in the state (as in the model node of create_agent), use stream_mode=[\"messages\", \"updates\"] to access completed messages through state updates (demonstrated below).\\nIf those messages are not tracked in the state, use custom updates or aggregate the chunks during the streaming loop (next section).\\n\\nRefer to the section below on streaming from sub-agents if your agent includes multiple LLMs.\\nCopyfrom typing import Any\\n\\nfrom langchain.agents import create_agent\\nfrom langchain.messages import AIMessage, AIMessageChunk, AnyMessage, ToolMessage\\n\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n\\n    return f\"It\\'s always sunny in {city}!\"\\n\\n\\nagent = create_agent(\"openai:gpt-5.2\", tools=[get_weather])\\n\\n\\ndef _render_message_chunk(token: AIMessageChunk) -> None:\\n    if token.text:\\n        print(token.text, end=\"|\")\\n    if token.tool_call_chunks:\\n        print(token.tool_call_chunks)\\n    # N.B. all content is available through token.content_blocks\\n\\n\\ndef _render_completed_message(message: AnyMessage) -> None:\\n    if isinstance(message, AIMessage) and message.tool_calls:\\n        print(f\"Tool calls: {message.tool_calls}\")\\n    if isinstance(message, ToolMessage):\\n        print(f\"Tool response: {message.content_blocks}\")\\n\\n\\ninput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\\nfor stream_mode, data in agent.stream(\\n    {\"messages\": [input_message]},\\n    stream_mode=[\"messages\", \"updates\"],  \\n):\\n    if stream_mode == \"messages\":\\n        token, metadata = data\\n        if isinstance(token, AIMessageChunk):\\n            _render_message_chunk(token)  \\n    if stream_mode == \"updates\":\\n        for source, update in data.items():\\n            if source in (\"model\", \"tools\"):  # `source` captures node name\\n                _render_completed_message(update[\"messages\"][-1])  \\n\\nOutputCopy[{\\'name\\': \\'get_weather\\', \\'args\\': \\'\\', \\'id\\': \\'call_D3Orjr89KgsLTZ9hTzYv7Hpf\\', \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'{\"\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'city\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'\":\"\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'Boston\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'\"}\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\nTool calls: [{\\'name\\': \\'get_weather\\', \\'args\\': {\\'city\\': \\'Boston\\'}, \\'id\\': \\'call_D3Orjr89KgsLTZ9hTzYv7Hpf\\', \\'type\\': \\'tool_call\\'}]\\nTool response: [{\\'type\\': \\'text\\', \\'text\\': \"It\\'s always sunny in Boston!\"}]\\nThe| weather| in| Boston| is| **|sun|ny|**|.|\\nSee all 9 lines\\n\\u200bAccessing completed messages\\nIf completed messages are tracked in an agent’s state, you can use stream_mode=[\"messages\", \"updates\"] as demonstrated above to access completed messages during streaming.\\nIn some cases, completed messages are not reflected in state updates. If you have access to the agent internals, you can use custom updates to access these messages during streaming. Otherwise, you can aggregate message chunks in the streaming loop (see below).\\nConsider the below example, where we incorporate a stream writer into a simplified guardrail middleware. This middleware demonstrates tool calling to generate a structured “safe / unsafe” evaluation (one could also use structured outputs for this):\\nCopyfrom typing import Any, Literal\\n\\nfrom langchain.agents.middleware import after_agent, AgentState\\nfrom langgraph.runtime import Runtime\\nfrom langchain.messages import AIMessage\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.config import get_stream_writer  \\nfrom pydantic import BaseModel\\n\\n\\nclass ResponseSafety(BaseModel):\\n    \"\"\"Evaluate a response as safe or unsafe.\"\"\"\\n    evaluation: Literal[\"safe\", \"unsafe\"]\\n\\n\\nsafety_model = init_chat_model(\"openai:gpt-5.2\")\\n\\n@after_agent(can_jump_to=[\"end\"])\\ndef safety_guardrail(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\\n    \"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\"\\n    stream_writer = get_stream_writer()  \\n    # Get the model response\\n    if not state[\"messages\"]:\\n        return None\\n\\n    last_message = state[\"messages\"][-1]\\n    if not isinstance(last_message, AIMessage):\\n        return None\\n\\n    # Use another model to evaluate safety\\n    model_with_tools = safety_model.bind_tools([ResponseSafety], tool_choice=\"any\")\\n    result = model_with_tools.invoke(\\n        [\\n            {\\n                \"role\": \"system\",\\n                \"content\": \"Evaluate this AI response as generally safe or unsafe.\",\\n            }\\n        ],\\n        {\"role\": \"user\", \"content\": f\"AI response: {last_message.text}\"},\\n    )\\n    stream_writer(result)  \\n\\n    tool_call = result.tool_calls[0]\\n    if tool_call[\"args\"][\"evaluation\"] == \"unsafe\":\\n        last_message.content = \"I cannot provide that response. Please rephrase your request.\"\\n\\n    return None\\n\\nWe can then incorporate this middleware into our agent and include its custom stream events:\\nCopyfrom typing import Any\\n\\nfrom langchain.agents import create_agent\\nfrom langchain.messages import AIMessageChunk, AIMessage, AnyMessage\\n\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n\\n    return f\"It\\'s always sunny in {city}!\"\\n\\n\\nagent = create_agent(\\n    model=\"openai:gpt-5.2\",\\n    tools=[get_weather],\\n    middleware=[safety_guardrail],  \\n)\\n\\ndef _render_message_chunk(token: AIMessageChunk) -> None:\\n    if token.text:\\n        print(token.text, end=\"|\")\\n    if token.tool_call_chunks:\\n        print(token.tool_call_chunks)\\n\\n\\ndef _render_completed_message(message: AnyMessage) -> None:\\n    if isinstance(message, AIMessage) and message.tool_calls:\\n        print(f\"Tool calls: {message.tool_calls}\")\\n    if isinstance(message, ToolMessage):\\n        print(f\"Tool response: {message.content_blocks}\")\\n\\n\\ninput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\\nfor stream_mode, data in agent.stream(\\n    {\"messages\": [input_message]},\\n    stream_mode=[\"messages\", \"updates\", \"custom\"],  \\n):\\n    if stream_mode == \"messages\":\\n        token, metadata = data\\n        if isinstance(token, AIMessageChunk):\\n            _render_message_chunk(token)\\n    if stream_mode == \"updates\":\\n        for source, update in data.items():\\n            if source in (\"model\", \"tools\"):\\n                _render_completed_message(update[\"messages\"][-1])\\n    if stream_mode == \"custom\":  \\n        # access completed message in stream\\n        print(f\"Tool calls: {data.tool_calls}\")  \\n\\nOutputCopy[{\\'name\\': \\'get_weather\\', \\'args\\': \\'\\', \\'id\\': \\'call_je6LWgxYzuZ84mmoDalTYMJC\\', \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'{\"\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'city\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'\":\"\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'Boston\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'\"}\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\nTool calls: [{\\'name\\': \\'get_weather\\', \\'args\\': {\\'city\\': \\'Boston\\'}, \\'id\\': \\'call_je6LWgxYzuZ84mmoDalTYMJC\\', \\'type\\': \\'tool_call\\'}]\\nTool response: [{\\'type\\': \\'text\\', \\'text\\': \"It\\'s always sunny in Boston!\"}]\\nThe| weather| in| **|Boston|**| is| **|sun|ny|**|.|[{\\'name\\': \\'ResponseSafety\\', \\'args\\': \\'\\', \\'id\\': \\'call_O8VJIbOG4Q9nQF0T8ltVi58O\\', \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'{\"\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'evaluation\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'\":\"\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'unsafe\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'\"}\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\nTool calls: [{\\'name\\': \\'ResponseSafety\\', \\'args\\': {\\'evaluation\\': \\'unsafe\\'}, \\'id\\': \\'call_O8VJIbOG4Q9nQF0T8ltVi58O\\', \\'type\\': \\'tool_call\\'}]\\nSee all 15 lines\\nAlternatively, if you aren’t able to add custom events to the stream, you can aggregate message chunks within the streaming loop:\\nCopyinput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\\nfull_message = None\\nfor stream_mode, data in agent.stream(\\n    {\"messages\": [input_message]},\\n    stream_mode=[\"messages\", \"updates\"],\\n):\\n    if stream_mode == \"messages\":\\n        token, metadata = data\\n        if isinstance(token, AIMessageChunk):\\n            _render_message_chunk(token)\\n            full_message = token if full_message is None else full_message + token  \\n            if token.chunk_position == \"last\":  \\n                if full_message.tool_calls:  \\n                    print(f\"Tool calls: {full_message.tool_calls}\")  \\n                full_message = None\\n    if stream_mode == \"updates\":\\n        for source, update in data.items():\\n            if source == \"tools\":\\n                _render_completed_message(update[\"messages\"][-1])\\n\\n\\u200bStreaming with human-in-the-loop\\nTo handle human-in-the-loop interrupts, we build on the above example:\\n\\nWe configure the agent with human-in-the-loop middleware and a checkpointer\\nWe collect interrupts generated during the \"updates\" stream mode\\nWe respond to those interrupts with a command\\n\\nCopyfrom typing import Any\\n\\nfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\\nfrom langchain.messages import AIMessage, AIMessageChunk, AnyMessage, ToolMessage\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.types import Command, Interrupt\\n\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n\\n    return f\"It\\'s always sunny in {city}!\"\\n\\n\\ncheckpointer = InMemorySaver()\\n\\nagent = create_agent(\\n    \"openai:gpt-5.2\",\\n    tools=[get_weather],\\n    middleware=[  \\n        HumanInTheLoopMiddleware(interrupt_on={\"get_weather\": True}),  \\n    ],  \\n    checkpointer=checkpointer,  \\n)\\n\\n\\ndef _render_message_chunk(token: AIMessageChunk) -> None:\\n    if token.text:\\n        print(token.text, end=\"|\")\\n    if token.tool_call_chunks:\\n        print(token.tool_call_chunks)\\n\\n\\ndef _render_completed_message(message: AnyMessage) -> None:\\n    if isinstance(message, AIMessage) and message.tool_calls:\\n        print(f\"Tool calls: {message.tool_calls}\")\\n    if isinstance(message, ToolMessage):\\n        print(f\"Tool response: {message.content_blocks}\")\\n\\n\\ndef _render_interrupt(interrupt: Interrupt) -> None:  \\n    interrupts = interrupt.value  \\n    for request in interrupts[\"action_requests\"]:  \\n        print(request[\"description\"])  \\n\\n\\ninput_message = {\\n    \"role\": \"user\",\\n    \"content\": (\\n        \"Can you look up the weather in Boston and San Francisco?\"\\n    ),\\n}\\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}  \\ninterrupts = []  \\nfor stream_mode, data in agent.stream(\\n    {\"messages\": [input_message]},\\n    config=config,  \\n    stream_mode=[\"messages\", \"updates\"],\\n):\\n    if stream_mode == \"messages\":\\n        token, metadata = data\\n        if isinstance(token, AIMessageChunk):\\n            _render_message_chunk(token)\\n    if stream_mode == \"updates\":\\n        for source, update in data.items():\\n            if source in (\"model\", \"tools\"):\\n                _render_completed_message(update[\"messages\"][-1])\\n            if source == \"__interrupt__\":  \\n                interrupts.extend(update)  \\n                _render_interrupt(update[0])  \\n\\nOutputCopy[{\\'name\\': \\'get_weather\\', \\'args\\': \\'\\', \\'id\\': \\'call_GOwNaQHeqMixay2qy80padfE\\', \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'{\"ci\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'ty\": \\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'\"Bosto\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'n\"}\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': \\'get_weather\\', \\'args\\': \\'\\', \\'id\\': \\'call_Ndb4jvWm2uMA0JDQXu37wDH6\\', \\'index\\': 1, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'{\"ci\\', \\'id\\': None, \\'index\\': 1, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'ty\": \\', \\'id\\': None, \\'index\\': 1, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'\"San F\\', \\'id\\': None, \\'index\\': 1, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'ranc\\', \\'id\\': None, \\'index\\': 1, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'isco\"\\', \\'id\\': None, \\'index\\': 1, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'}\\', \\'id\\': None, \\'index\\': 1, \\'type\\': \\'tool_call_chunk\\'}]\\nTool calls: [{\\'name\\': \\'get_weather\\', \\'args\\': {\\'city\\': \\'Boston\\'}, \\'id\\': \\'call_GOwNaQHeqMixay2qy80padfE\\', \\'type\\': \\'tool_call\\'}, {\\'name\\': \\'get_weather\\', \\'args\\': {\\'city\\': \\'San Francisco\\'}, \\'id\\': \\'call_Ndb4jvWm2uMA0JDQXu37wDH6\\', \\'type\\': \\'tool_call\\'}]\\nTool execution requires approval\\n\\nTool: get_weather\\nArgs: {\\'city\\': \\'Boston\\'}\\nTool execution requires approval\\n\\nTool: get_weather\\nArgs: {\\'city\\': \\'San Francisco\\'}\\nSee all 21 lines\\nWe next collect a decision for each interrupt. Importantly, the order of decisions must match the order of actions we collected.\\nTo illustrate, we will edit one tool call and accept the other:\\nCopydef _get_interrupt_decisions(interrupt: Interrupt) -> list[dict]:\\n    return [\\n        {\\n            \"type\": \"edit\",\\n            \"edited_action\": {\\n                \"name\": \"get_weather\",\\n                \"args\": {\"city\": \"Boston, U.K.\"},\\n            },\\n        }\\n        if \"boston\" in request[\"description\"].lower()\\n        else {\"type\": \"approve\"}\\n        for request in interrupt.value[\"action_requests\"]\\n    ]\\n\\ndecisions = {}\\nfor interrupt in interrupts:\\n    decisions[interrupt.id] = {\\n        \"decisions\": _get_interrupt_decisions(interrupt)\\n    }\\n\\ndecisions\\n\\nOutputCopy{\\n    \\'a96c40474e429d661b5b32a8d86f0f3e\\': {\\n        \\'decisions\\': [\\n            {\\n                \\'type\\': \\'edit\\',\\n                 \\'edited_action\\': {\\n                     \\'name\\': \\'get_weather\\',\\n                     \\'args\\': {\\'city\\': \\'Boston, U.K.\\'}\\n                 }\\n            },\\n            {\\'type\\': \\'approve\\'},\\n        ]\\n    }\\n}\\n\\nWe can then resume by passing a command into the same streaming loop:\\nCopyinterrupts = []\\nfor stream_mode, data in agent.stream(\\n    Command(resume=decisions),  \\n    config=config,\\n    stream_mode=[\"messages\", \"updates\"],\\n):\\n    # Streaming loop is unchanged\\n    if stream_mode == \"messages\":\\n        token, metadata = data\\n        if isinstance(token, AIMessageChunk):\\n            _render_message_chunk(token)\\n    if stream_mode == \"updates\":\\n        for source, update in data.items():\\n            if source in (\"model\", \"tools\"):\\n                _render_completed_message(update[\"messages\"][-1])\\n            if source == \"__interrupt__\":\\n                interrupts.extend(update)\\n                _render_interrupt(update[0])\\n\\nOutputCopyTool response: [{\\'type\\': \\'text\\', \\'text\\': \"It\\'s always sunny in Boston, U.K.!\"}]\\nTool response: [{\\'type\\': \\'text\\', \\'text\\': \"It\\'s always sunny in San Francisco!\"}]\\n-| **|Boston|**|:| It|’s| always| sunny| in| Boston|,| U|.K|.|\\n|-| **|San| Francisco|**|:| It|’s| always| sunny| in| San| Francisco|!|\\n\\n\\u200bStreaming from sub-agents\\nWhen there are multiple LLMs at any point in an agent, it’s often necessary to disambiguate the source of messages as they are generated.\\nTo do this, you can initialize any model with tags. These tags are then available in metadata when streaming in \"messages\" mode.\\nBelow, we update the streaming tool calls example:\\n\\nWe replace our tool with a call_weather_agent tool that invokes an agent internally\\nWe add a string tag to this LLM and the outer “supervisor” LLM\\nWe specify subgraphs=True when creating the stream\\nOur stream processing is identical to before, but we add logic to keep track of what LLM is active\\n\\nIf streaming tokens from sub-agents is not needed, you can initialize the sub-agent with a name. This name is accessible on messages generated by the sub-agent when streaming updates.\\nFirst we construct the agent:\\nCopyfrom typing import Any\\n\\nfrom langchain.agents import create_agent\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.messages import AIMessage, AnyMessage\\n\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n\\n    return f\"It\\'s always sunny in {city}!\"\\n\\n\\nweather_model = init_chat_model(\\n    \"openai:gpt-5.2\",\\n    tags=[\"weather_sub_agent\"],\\n)\\n\\nweather_agent = create_agent(model=weather_model, tools=[get_weather])\\n\\n\\ndef call_weather_agent(query: str) -> str:\\n    \"\"\"Query the weather agent.\"\"\"\\n    result = weather_agent.invoke({\\n        \"messages\": [{\"role\": \"user\", \"content\": query}]\\n    })\\n    return result[\"messages\"][-1].text\\n\\n\\nsupervisor_model = init_chat_model(\\n    \"openai:gpt-5.2\",\\n    tags=[\"supervisor\"],\\n)\\n\\nagent = create_agent(model=supervisor_model, tools=[call_weather_agent])\\n\\nNext, we add logic to the streaming loop to report which agent is emitting tokens:\\nCopydef _render_message_chunk(token: AIMessageChunk) -> None:\\n    if token.text:\\n        print(token.text, end=\"|\")\\n    if token.tool_call_chunks:\\n        print(token.tool_call_chunks)\\n\\n\\ndef _render_completed_message(message: AnyMessage) -> None:\\n    if isinstance(message, AIMessage) and message.tool_calls:\\n        print(f\"Tool calls: {message.tool_calls}\")\\n    if isinstance(message, ToolMessage):\\n        print(f\"Tool response: {message.content_blocks}\")\\n\\n\\ninput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\\ncurrent_agent = None\\nfor _, stream_mode, data in agent.stream(\\n    {\"messages\": [input_message]},\\n    stream_mode=[\"messages\", \"updates\"],\\n    subgraphs=True,  \\n):\\n    if stream_mode == \"messages\":\\n        token, metadata = data\\n        if tags := metadata.get(\"tags\", []):  \\n            this_agent = tags[0]  \\n            if this_agent != current_agent:  \\n                print(f\"🤖 {this_agent}: \")  \\n                current_agent = this_agent  \\n        if isinstance(token, AIMessage):\\n            _render_message_chunk(token)\\n    if stream_mode == \"updates\":\\n        for source, update in data.items():\\n            if source in (\"model\", \"tools\"):\\n                _render_completed_message(update[\"messages\"][-1])\\n\\nOutputCopy🤖 supervisor:\\n[{\\'name\\': \\'call_weather_agent\\', \\'args\\': \\'\\', \\'id\\': \\'call_asorzUf0mB6sb7MiKfgojp7I\\', \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'{\"\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'query\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'\":\"\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'Boston\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\' weather\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\' right\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\' now\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\' and\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \" today\\'s\", \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\' forecast\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'\"}\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\nTool calls: [{\\'name\\': \\'call_weather_agent\\', \\'args\\': {\\'query\\': \"Boston weather right now and today\\'s forecast\"}, \\'id\\': \\'call_asorzUf0mB6sb7MiKfgojp7I\\', \\'type\\': \\'tool_call\\'}]\\n🤖 weather_sub_agent:\\n[{\\'name\\': \\'get_weather\\', \\'args\\': \\'\\', \\'id\\': \\'call_LZ89lT8fW6w8vqck5pZeaDIx\\', \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'{\"\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'city\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'\":\"\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'Boston\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\n[{\\'name\\': None, \\'args\\': \\'\"}\\', \\'id\\': None, \\'index\\': 0, \\'type\\': \\'tool_call_chunk\\'}]\\nTool calls: [{\\'name\\': \\'get_weather\\', \\'args\\': {\\'city\\': \\'Boston\\'}, \\'id\\': \\'call_LZ89lT8fW6w8vqck5pZeaDIx\\', \\'type\\': \\'tool_call\\'}]\\nTool response: [{\\'type\\': \\'text\\', \\'text\\': \"It\\'s always sunny in Boston!\"}]\\nBoston| weather| right| now|:| **|Sunny|**|.\\n\\n|Today|’s| forecast| for| Boston|:| **|Sunny| all| day|**|.|Tool response: [{\\'type\\': \\'text\\', \\'text\\': \\'Boston weather right now: **Sunny**.\\\\n\\\\nToday’s forecast for Boston: **Sunny all day**.\\'}]\\n🤖 supervisor:\\nBoston| weather| right| now|:| **|Sunny|**|.\\n\\n|Today|’s| forecast| for| Boston|:| **|Sunny| all| day|**|.|\\nSee all 30 lines\\n\\u200bDisable streaming\\nIn some applications you might need to disable streaming of individual tokens for a given model. This is useful when:\\n\\nWorking with multi-agent systems to control which agents stream their output\\nMixing models that support streaming with those that do not\\nDeploying to LangSmith and wanting to prevent certain model outputs from being streamed to the client\\n\\nSet streaming=False when initializing the model.\\nCopyfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(\\n    model=\"gpt-4o\",\\n    streaming=False\\n)\\n\\nWhen deploying to LangSmith, set streaming=False on any models whose output you don’t want streamed to the client. This is configured in your graph code before deployment.\\nNot all chat model integrations support the streaming parameter. If your model doesn’t support it, use disable_streaming=True instead. This parameter is available on all chat models via the base class.\\nSee the LangGraph streaming guide for more details.\\n\\u200bRelated\\n\\nStreaming with chat models — Stream tokens directly from a chat model without using an agent or graph\\nStreaming with human-in-the-loop — Stream agent progress while handling interrupts for human review\\nLangGraph streaming — Advanced streaming options including values, debug modes, and subgraph streaming\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoShort-term memoryPreviousStructured outputNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_urls=['https://docs.langchain.com/oss/python/langchain/agents',\n",
    "'https://docs.langchain.com/oss/python/langchain/tools',\n",
    "'https://docs.langchain.com/oss/python/langchain/streaming']\n",
    "\n",
    "lc_docs = WebBaseLoader(web_paths=lc_urls).load()\n",
    "lc_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c9a6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of langchain chunks: 87\n"
     ]
    }
   ],
   "source": [
    "lc_chunks = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100).split_documents(lc_docs)\n",
    "print(f\"Number of langchain chunks: {len(lc_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55c8e723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='425c94a0-99e2-4a1d-8ddb-2e837173dbcf', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\\n\\u200bOverview\\nLangChain’s streaming system lets you surface live feedback from agent runs to your application.\\nWhat’s possible with LangChain streaming:'),\n",
       " Document(id='912ce9fc-5f3e-4c08-9ddb-fe661ccab5c2', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='Streaming - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsStreamingLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageOverviewSupported stream modesAgent progressLLM tokensCustom updatesStream multiple modesCommon patternsStreaming tool callsAccessing completed messagesStreaming with human-in-the-loopStreaming from sub-agentsDisable streamingRelatedCore componentsStreamingCopy pageCopy pageLangChain implements a streaming'),\n",
       " Document(id='780a7f0c-50f4-49a9-9b8c-204e6bda412c', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='streamingRelatedCore componentsStreamingCopy pageCopy pageLangChain implements a streaming system to surface real-time updates.'),\n",
       " Document(id='10853735-6ca8-4d62-bc8d-33fa142c82de', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='model = ChatOpenAI(\\n    model=\"gpt-4o\",\\n    streaming=False\\n)\\n\\nWhen deploying to LangSmith, set streaming=False on any models whose output you don’t want streamed to the client. This is configured in your graph code before deployment.\\nNot all chat model integrations support the streaming parameter. If your model doesn’t support it, use disable_streaming=True instead. This parameter is available on all chat models via the base class.\\nSee the LangGraph streaming guide for more details.\\n\\u200bRelated\\n\\nStreaming with chat models — Stream tokens directly from a chat model without using an agent or graph\\nStreaming with human-in-the-loop — Stream agent progress while handling interrupts for human review\\nLangGraph streaming — Advanced streaming options including values, debug modes, and subgraph streaming')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "    documents=lc_chunks,\n",
    "    embedding=HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    ")\n",
    "\n",
    "lc_retriever = vectorstore.as_retriever()\n",
    "lc_retriever.invoke(\"what's possible with langchain streaming?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba40e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating retriever tool\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "langchain_tool = create_retriever_tool(\n",
    "    retriever=lc_retriever,\n",
    "    name=\"langchain retriever\",\n",
    "    description=\"this tool retrieves relevant documents about langchain based on query\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e87c94",
   "metadata": {},
   "source": [
    "#### Langgraph workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd06bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, List\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages : Annotated[Sequence, add_messages]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbb1e8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x0000023B54D283D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000023B54E82010>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model='llama-3.1-8b-instant')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98e14596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining nodes\n",
    "\n",
    "def agent(state:AgentState):\n",
    "    \"\"\"Invokes the agent model to generate a response based on current state.\n",
    "    Given the question, it will decide to retrieve using retriever tool, or simply end.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--CALL AGENT--\")\n",
    "    tools_model = model.bind_tools([langgraph_tool, langchain_tool])\n",
    "    response = tools_model.invoke(state['messages'])\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40e692ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining class for structured output\n",
    "from typing import Literal\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "class Relevancy(TypedDict):\n",
    "    score : Literal['yes', 'no']\n",
    "\n",
    "#defining grader node\n",
    "\n",
    "def grader(state:AgentState)-> Literal['generate', 'rewrite']:\n",
    "    \"\"\"This function grades the relevant documents fetched by agent and returns defined literals accordingly\"\"\" \n",
    "    context = state['messages'][-1].content\n",
    "    user_query = state['messages'][0].content\n",
    "\n",
    "    relevancy_grader_model = model.with_structured_output(Relevancy)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=['context', 'user_query'],\n",
    "        template='''you are the grader assessing relevance of a retrieved documents to a user question.\n",
    "        here is the retrieved documents: \\n\\n {context}\n",
    "        here is the user question: {user_query}\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "'''\n",
    "    )\n",
    "\n",
    "    grader_chain = ({\"context\": context, \"user_query\":user_query} |\n",
    "                    prompt |\n",
    "                    relevancy_grader_model)\n",
    "    \n",
    "    relevancy_score = grader_chain.score\n",
    "\n",
    "    if relevancy_score==\"yes\":\n",
    "        print(\"--DOCS ARE RELEVANT, GENERATE CALLED--\")\n",
    "        return 'generate'\n",
    "    else:\n",
    "        print(\"--DOCS ARE IRRELEVANT, REWRITE CALLED--\")\n",
    "        return \"rewrite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining generate and rewrite node\n",
    "\n",
    "def generate(state:AgentState):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGs In Depth (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
