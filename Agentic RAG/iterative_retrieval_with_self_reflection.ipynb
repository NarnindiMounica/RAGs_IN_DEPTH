{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c702d85",
   "metadata": {},
   "source": [
    "### Iterative Retrieval with Self Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3086d2c4",
   "metadata": {},
   "source": [
    "Iterative retrieval is a dynamic strategy where an AI agent doesn't settle for the first batch of retrieved documents. Instead, it evaluates the adequacy of the initial context, and if necessary it:\n",
    "* refines the query\n",
    "* retrieves again\n",
    "* repeats the process until it's confident enough to answer the original question\n",
    "\n",
    "In iterative RAG:\n",
    "* the agent reflects on the retrieved content and the answer it  produced.\n",
    "* it it's unsure, it can refine it's search (like a human researcher would)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "137888c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAGs In Depth\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c542a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=['https://docs.langchain.com/oss/python/langchain/agents',\n",
    "'https://docs.langchain.com/oss/python/langchain/tools',\n",
    "'https://docs.langchain.com/oss/python/langchain/streaming']\n",
    "\n",
    "documents = WebBaseLoader(web_paths=urls).load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100).split_documents(documents)\n",
    "embeddings = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "retriever = FAISS.from_documents(documents=chunks,\n",
    "                                 embedding=embeddings).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc08caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001C40479DC90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C4046EFA10>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#base model\n",
    "\n",
    "model = init_chat_model(model=\"groq:llama-3.1-8b-instant\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eced616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining state schema\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class IterativeRAGState(BaseModel):\n",
    "    question:str\n",
    "    refined_question: str = \"\"\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer: str = \"\"\n",
    "    verified: bool = False\n",
    "    attempts: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining nodes\n",
    "from typing_extensions import TypedDict\n",
    "from langchain.agents import create_agent\n",
    "#retrieving docs node\n",
    "\n",
    "def retrieve_docs(state:IterativeRAGState)->IterativeRAGState:\n",
    "    #this node retrieves relevant docs for the given question\n",
    "    relevant_docs = retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"retrieved_docs\": relevant_docs})\n",
    "\n",
    "#generating answer node\n",
    "\n",
    "class Context(TypedDict):\n",
    "    context: str\n",
    "\n",
    "def generate_answer(state:IterativeRAGState)->IterativeRAGState:\n",
    "    #this node generates response based on given context information\n",
    "    combined_doc = \"\\n\\n\".join([doc.page_content for doc in state.retrieved_docs])\n",
    "    prompt = \"generate answer to the given question only using the context information provided\"\n",
    "    gen_answer_agent = create_agent(model=model,\n",
    "                                    system_prompt=prompt,\n",
    "                                    context_schema=Context)\n",
    "    gen_answer_response = gen_answer_agent.invoke({\"messages\":[{\"role\":\"user\", \"content\": state.question}]}, context=combined_doc)\n",
    "    return state.model_copy(update={\"answer\": gen_answer_agent['messages'][-1].content})\n",
    "\n",
    "#answer verifying node\n",
    "from langchain.agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGs In Depth (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
