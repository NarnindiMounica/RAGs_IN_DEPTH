{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc888bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of langchain chunks: 87\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "lc_urls=['https://docs.langchain.com/oss/python/langchain/agents',\n",
    "'https://docs.langchain.com/oss/python/langchain/tools',\n",
    "'https://docs.langchain.com/oss/python/langchain/streaming']\n",
    "\n",
    "lc_docs = WebBaseLoader(web_paths=lc_urls).load()\n",
    "lc_chunks = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100).split_documents(lc_docs)\n",
    "print(f\"Number of langchain chunks: {len(lc_chunks)}\")\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=lc_chunks,\n",
    "    embedding=HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    ")\n",
    "\n",
    "lc_retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52271f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='cfc0e1ae-8858-4cf5-9baf-b7fb74edf53e', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='create_agent provides a production-ready agent implementation.\\nAn LLM Agent runs tools in a loop to achieve a goal.\\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.'),\n",
       " Document(id='90b5f040-77ef-4e53-a246-50643ea48c25', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='create_agent builds a graph-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.Learn more about the Graph API.\\n\\u200bCore components\\n\\u200bModel\\nThe model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\\n\\u200bStatic model\\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\\nTo initialize a static model from a model identifier string:\\nCopyfrom langchain.agents import create_agent\\n\\nagent = create_agent(\"gpt-5\", tools=tools)'),\n",
       " Document(id='49160b7b-2064-49bf-9978-d7507165fc94', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='For more information, see Tools.\\n\\u200bDefining tools\\nPass a list of tools to the agent.\\nTools can be specified as plain Python functions or coroutines.The tool decorator can be used to customize tool names, descriptions, argument schemas, and other properties.\\nCopyfrom langchain.tools import tool\\nfrom langchain.agents import create_agent\\n\\n\\n@tool\\ndef search(query: str) -> str:\\n    \"\"\"Search for information.\"\"\"\\n    return f\"Results for: {query}\"\\n\\n@tool\\ndef get_weather(location: str) -> str:\\n    \"\"\"Get weather information for a location.\"\"\"\\n    return f\"Weather in {location}: Sunny, 72°F\"\\n\\nagent = create_agent(model, tools=[search, get_weather])'),\n",
       " Document(id='f2da71de-8f89-4e0c-89d1-5d9f520a6361', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/agents', 'title': 'Agents - Docs by LangChain', 'language': 'en'}, page_content='agent = create_agent(\\n    model=\"gpt-4o\",\\n    response_format=ProviderStrategy(ContactInfo)\\n)\\n\\nAs of langchain 1.0, simply passing a schema (e.g., response_format=ContactInfo) is no longer supported. You must explicitly use ToolStrategy or ProviderStrategy.\\nTo learn about structured output, see Structured output.\\n\\u200bMemory\\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\\nInformation stored in the state can be thought of as the short-term memory of the agent:\\nCustom state schemas must extend AgentState as a TypedDict.\\nThere are two ways to define custom state:\\n\\nVia middleware (preferred)\\nVia state_schema on create_agent')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def langchain_tool(query:str):\n",
    "    \"this function retrieves relevant documents about langchain based on query\"\n",
    "    related_lc_docs =lc_retriever.invoke(query)\n",
    "    return related_lc_docs\n",
    "\n",
    "tools = [langchain_tool]\n",
    "langchain_tool(\"what is create_agent function used for?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38f5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000024E1B020090>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.agents import create_agent\n",
    "model = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "     messages: Annotated[List, add_messages]\n",
    "\n",
    "\n",
    "# def node(state:State):\n",
    "#     tools_model = create_agent(model=model, tools=tools, response_format=State)\n",
    "#     res = tools_model.invoke({\"messages\":[{\"role\":\"user\", \"content\": state['messages']}]})\n",
    "#     return {\"messages\": res}  \n",
    "# \n",
    "\n",
    "agent=create_agent(model=model, tools=tools, debug=True) \n",
    "def node(state:State):\n",
    "     res=agent.invoke({\"messages\":state['messages']})\n",
    "     return {\"messages\": res['messages'][-1].content}\n",
    "\n",
    "gr = StateGraph(State)\n",
    "\n",
    "gr.add_node(\"node\", node)\n",
    "gr.add_edge(START, \"node\")\n",
    "\n",
    "gb = gr.compile()\n",
    "gb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='streaming in langchain', additional_kwargs={}, response_metadata={}, id='121e39be-6d2d-46bd-ba53-6bda05a60db0')]}\n",
      "\u001b[1m[updates]\u001b[0m {'model': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'qs5xrme79', 'function': {'arguments': '{\"query\":\"langchain streaming\"}', 'name': 'langchain_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 229, 'total_tokens': 247, 'completion_time': 0.020119333, 'completion_tokens_details': None, 'prompt_time': 0.014904553, 'prompt_tokens_details': None, 'queue_time': 0.049959527, 'total_time': 0.035023886}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b4db8-5a4d-7a32-962f-1149e899c50c-0', tool_calls=[{'name': 'langchain_tool', 'args': {'query': 'langchain streaming'}, 'id': 'qs5xrme79', 'type': 'tool_call'}], usage_metadata={'input_tokens': 229, 'output_tokens': 18, 'total_tokens': 247})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='streaming in langchain', additional_kwargs={}, response_metadata={}, id='121e39be-6d2d-46bd-ba53-6bda05a60db0'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'qs5xrme79', 'function': {'arguments': '{\"query\":\"langchain streaming\"}', 'name': 'langchain_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 229, 'total_tokens': 247, 'completion_time': 0.020119333, 'completion_tokens_details': None, 'prompt_time': 0.014904553, 'prompt_tokens_details': None, 'queue_time': 0.049959527, 'total_time': 0.035023886}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b4db8-5a4d-7a32-962f-1149e899c50c-0', tool_calls=[{'name': 'langchain_tool', 'args': {'query': 'langchain streaming'}, 'id': 'qs5xrme79', 'type': 'tool_call'}], usage_metadata={'input_tokens': 229, 'output_tokens': 18, 'total_tokens': 247})]}\n",
      "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content='[Document(id=\\'41eeefb5-e29f-46a7-814d-7e7a2fac1fc2\\', metadata={\\'source\\': \\'https://docs.langchain.com/oss/python/langchain/streaming\\', \\'title\\': \\'Streaming - Docs by LangChain\\', \\'language\\': \\'en\\'}, page_content=\\'Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\\\\n\\\\u200bOverview\\\\nLangChain’s streaming system lets you surface live feedback from agent runs to your application.\\\\nWhat’s possible with LangChain streaming:\\'), Document(id=\\'553e47b2-4496-4dd3-9ded-d6947b2447f7\\', metadata={\\'source\\': \\'https://docs.langchain.com/oss/python/langchain/streaming\\', \\'title\\': \\'Streaming - Docs by LangChain\\', \\'language\\': \\'en\\'}, page_content=\\'Streaming - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsStreamingLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageOverviewSupported stream modesAgent progressLLM tokensCustom updatesStream multiple modesCommon patternsStreaming tool callsAccessing completed messagesStreaming with human-in-the-loopStreaming from sub-agentsDisable streamingRelatedCore componentsStreamingCopy pageCopy pageLangChain implements a streaming\\'), Document(id=\\'734a4205-8a16-457c-b8cc-3d0b449c1cf8\\', metadata={\\'source\\': \\'https://docs.langchain.com/oss/python/langchain/streaming\\', \\'title\\': \\'Streaming - Docs by LangChain\\', \\'language\\': \\'en\\'}, page_content=\\'streamingRelatedCore componentsStreamingCopy pageCopy pageLangChain implements a streaming system to surface real-time updates.\\'), Document(id=\\'282bcfbe-a3e4-4b3a-8e39-5fe87a8e75eb\\', metadata={\\'source\\': \\'https://docs.langchain.com/oss/python/langchain/streaming\\', \\'title\\': \\'Streaming - Docs by LangChain\\', \\'language\\': \\'en\\'}, page_content=\\'model = ChatOpenAI(\\\\n    model=\"gpt-4o\",\\\\n    streaming=False\\\\n)\\\\n\\\\nWhen deploying to LangSmith, set streaming=False on any models whose output you don’t want streamed to the client. This is configured in your graph code before deployment.\\\\nNot all chat model integrations support the streaming parameter. If your model doesn’t support it, use disable_streaming=True instead. This parameter is available on all chat models via the base class.\\\\nSee the LangGraph streaming guide for more details.\\\\n\\\\u200bRelated\\\\n\\\\nStreaming with chat models — Stream tokens directly from a chat model without using an agent or graph\\\\nStreaming with human-in-the-loop — Stream agent progress while handling interrupts for human review\\\\nLangGraph streaming — Advanced streaming options including values, debug modes, and subgraph streaming\\')]', name='langchain_tool', id='24fd0ffc-facd-4e64-88c1-c92ebe790cef', tool_call_id='qs5xrme79')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='streaming in langchain', additional_kwargs={}, response_metadata={}, id='121e39be-6d2d-46bd-ba53-6bda05a60db0'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'qs5xrme79', 'function': {'arguments': '{\"query\":\"langchain streaming\"}', 'name': 'langchain_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 229, 'total_tokens': 247, 'completion_time': 0.020119333, 'completion_tokens_details': None, 'prompt_time': 0.014904553, 'prompt_tokens_details': None, 'queue_time': 0.049959527, 'total_time': 0.035023886}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b4db8-5a4d-7a32-962f-1149e899c50c-0', tool_calls=[{'name': 'langchain_tool', 'args': {'query': 'langchain streaming'}, 'id': 'qs5xrme79', 'type': 'tool_call'}], usage_metadata={'input_tokens': 229, 'output_tokens': 18, 'total_tokens': 247}), ToolMessage(content='[Document(id=\\'41eeefb5-e29f-46a7-814d-7e7a2fac1fc2\\', metadata={\\'source\\': \\'https://docs.langchain.com/oss/python/langchain/streaming\\', \\'title\\': \\'Streaming - Docs by LangChain\\', \\'language\\': \\'en\\'}, page_content=\\'Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\\\\n\\\\u200bOverview\\\\nLangChain’s streaming system lets you surface live feedback from agent runs to your application.\\\\nWhat’s possible with LangChain streaming:\\'), Document(id=\\'553e47b2-4496-4dd3-9ded-d6947b2447f7\\', metadata={\\'source\\': \\'https://docs.langchain.com/oss/python/langchain/streaming\\', \\'title\\': \\'Streaming - Docs by LangChain\\', \\'language\\': \\'en\\'}, page_content=\\'Streaming - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsStreamingLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageOverviewSupported stream modesAgent progressLLM tokensCustom updatesStream multiple modesCommon patternsStreaming tool callsAccessing completed messagesStreaming with human-in-the-loopStreaming from sub-agentsDisable streamingRelatedCore componentsStreamingCopy pageCopy pageLangChain implements a streaming\\'), Document(id=\\'734a4205-8a16-457c-b8cc-3d0b449c1cf8\\', metadata={\\'source\\': \\'https://docs.langchain.com/oss/python/langchain/streaming\\', \\'title\\': \\'Streaming - Docs by LangChain\\', \\'language\\': \\'en\\'}, page_content=\\'streamingRelatedCore componentsStreamingCopy pageCopy pageLangChain implements a streaming system to surface real-time updates.\\'), Document(id=\\'282bcfbe-a3e4-4b3a-8e39-5fe87a8e75eb\\', metadata={\\'source\\': \\'https://docs.langchain.com/oss/python/langchain/streaming\\', \\'title\\': \\'Streaming - Docs by LangChain\\', \\'language\\': \\'en\\'}, page_content=\\'model = ChatOpenAI(\\\\n    model=\"gpt-4o\",\\\\n    streaming=False\\\\n)\\\\n\\\\nWhen deploying to LangSmith, set streaming=False on any models whose output you don’t want streamed to the client. This is configured in your graph code before deployment.\\\\nNot all chat model integrations support the streaming parameter. If your model doesn’t support it, use disable_streaming=True instead. This parameter is available on all chat models via the base class.\\\\nSee the LangGraph streaming guide for more details.\\\\n\\\\u200bRelated\\\\n\\\\nStreaming with chat models — Stream tokens directly from a chat model without using an agent or graph\\\\nStreaming with human-in-the-loop — Stream agent progress while handling interrupts for human review\\\\nLangGraph streaming — Advanced streaming options including values, debug modes, and subgraph streaming\\')]', name='langchain_tool', id='24fd0ffc-facd-4e64-88c1-c92ebe790cef', tool_call_id='qs5xrme79')]}\n",
      "\u001b[1m[updates]\u001b[0m {'model': {'messages': [AIMessage(content=\"The langchain_tool function retrieved the following information about streaming in langchain:\\n\\n1. LangChain's streaming system allows for live feedback from agent runs to be displayed in applications, improving user experience.\\n2. Streaming is crucial for enhancing responsiveness in applications built on LLMs.\\n3. LangChain implements a streaming system to surface real-time updates.\\n4. There are different stream modes available, including agent progress and LLM tokens.\\n5. Custom updates can be streamed by using the `streaming` parameter in the `ChatOpenAI` model.\\n6. Some chat model integrations support the `streaming` parameter, while others require using `disable_streaming=True`.\\n7. LangGraph streaming offers advanced options, including values, debug modes, and subgraph streaming.\\n\\nThis information provides a comprehensive overview of streaming in langchain and its various applications, including its importance, implementation, and customization options.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 182, 'prompt_tokens': 984, 'total_tokens': 1166, 'completion_time': 0.280454418, 'completion_tokens_details': None, 'prompt_time': 0.058672436, 'prompt_tokens_details': None, 'queue_time': 0.057833454, 'total_time': 0.339126854}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b4db8-5b43-77b3-a2c7-ee6bfe62cb40-0', usage_metadata={'input_tokens': 984, 'output_tokens': 182, 'total_tokens': 1166})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='streaming in langchain', additional_kwargs={}, response_metadata={}, id='121e39be-6d2d-46bd-ba53-6bda05a60db0'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'qs5xrme79', 'function': {'arguments': '{\"query\":\"langchain streaming\"}', 'name': 'langchain_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 229, 'total_tokens': 247, 'completion_time': 0.020119333, 'completion_tokens_details': None, 'prompt_time': 0.014904553, 'prompt_tokens_details': None, 'queue_time': 0.049959527, 'total_time': 0.035023886}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b4db8-5a4d-7a32-962f-1149e899c50c-0', tool_calls=[{'name': 'langchain_tool', 'args': {'query': 'langchain streaming'}, 'id': 'qs5xrme79', 'type': 'tool_call'}], usage_metadata={'input_tokens': 229, 'output_tokens': 18, 'total_tokens': 247}), ToolMessage(content='[Document(id=\\'41eeefb5-e29f-46a7-814d-7e7a2fac1fc2\\', metadata={\\'source\\': \\'https://docs.langchain.com/oss/python/langchain/streaming\\', \\'title\\': \\'Streaming - Docs by LangChain\\', \\'language\\': \\'en\\'}, page_content=\\'Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\\\\n\\\\u200bOverview\\\\nLangChain’s streaming system lets you surface live feedback from agent runs to your application.\\\\nWhat’s possible with LangChain streaming:\\'), Document(id=\\'553e47b2-4496-4dd3-9ded-d6947b2447f7\\', metadata={\\'source\\': \\'https://docs.langchain.com/oss/python/langchain/streaming\\', \\'title\\': \\'Streaming - Docs by LangChain\\', \\'language\\': \\'en\\'}, page_content=\\'Streaming - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsStreamingLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageOverviewSupported stream modesAgent progressLLM tokensCustom updatesStream multiple modesCommon patternsStreaming tool callsAccessing completed messagesStreaming with human-in-the-loopStreaming from sub-agentsDisable streamingRelatedCore componentsStreamingCopy pageCopy pageLangChain implements a streaming\\'), Document(id=\\'734a4205-8a16-457c-b8cc-3d0b449c1cf8\\', metadata={\\'source\\': \\'https://docs.langchain.com/oss/python/langchain/streaming\\', \\'title\\': \\'Streaming - Docs by LangChain\\', \\'language\\': \\'en\\'}, page_content=\\'streamingRelatedCore componentsStreamingCopy pageCopy pageLangChain implements a streaming system to surface real-time updates.\\'), Document(id=\\'282bcfbe-a3e4-4b3a-8e39-5fe87a8e75eb\\', metadata={\\'source\\': \\'https://docs.langchain.com/oss/python/langchain/streaming\\', \\'title\\': \\'Streaming - Docs by LangChain\\', \\'language\\': \\'en\\'}, page_content=\\'model = ChatOpenAI(\\\\n    model=\"gpt-4o\",\\\\n    streaming=False\\\\n)\\\\n\\\\nWhen deploying to LangSmith, set streaming=False on any models whose output you don’t want streamed to the client. This is configured in your graph code before deployment.\\\\nNot all chat model integrations support the streaming parameter. If your model doesn’t support it, use disable_streaming=True instead. This parameter is available on all chat models via the base class.\\\\nSee the LangGraph streaming guide for more details.\\\\n\\\\u200bRelated\\\\n\\\\nStreaming with chat models — Stream tokens directly from a chat model without using an agent or graph\\\\nStreaming with human-in-the-loop — Stream agent progress while handling interrupts for human review\\\\nLangGraph streaming — Advanced streaming options including values, debug modes, and subgraph streaming\\')]', name='langchain_tool', id='24fd0ffc-facd-4e64-88c1-c92ebe790cef', tool_call_id='qs5xrme79'), AIMessage(content=\"The langchain_tool function retrieved the following information about streaming in langchain:\\n\\n1. LangChain's streaming system allows for live feedback from agent runs to be displayed in applications, improving user experience.\\n2. Streaming is crucial for enhancing responsiveness in applications built on LLMs.\\n3. LangChain implements a streaming system to surface real-time updates.\\n4. There are different stream modes available, including agent progress and LLM tokens.\\n5. Custom updates can be streamed by using the `streaming` parameter in the `ChatOpenAI` model.\\n6. Some chat model integrations support the `streaming` parameter, while others require using `disable_streaming=True`.\\n7. LangGraph streaming offers advanced options, including values, debug modes, and subgraph streaming.\\n\\nThis information provides a comprehensive overview of streaming in langchain and its various applications, including its importance, implementation, and customization options.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 182, 'prompt_tokens': 984, 'total_tokens': 1166, 'completion_time': 0.280454418, 'completion_tokens_details': None, 'prompt_time': 0.058672436, 'prompt_tokens_details': None, 'queue_time': 0.057833454, 'total_time': 0.339126854}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b4db8-5b43-77b3-a2c7-ee6bfe62cb40-0', usage_metadata={'input_tokens': 984, 'output_tokens': 182, 'total_tokens': 1166})]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='streaming in langchain', additional_kwargs={}, response_metadata={}, id='121e39be-6d2d-46bd-ba53-6bda05a60db0'),\n",
       "  HumanMessage(content=\"The langchain_tool function retrieved the following information about streaming in langchain:\\n\\n1. LangChain's streaming system allows for live feedback from agent runs to be displayed in applications, improving user experience.\\n2. Streaming is crucial for enhancing responsiveness in applications built on LLMs.\\n3. LangChain implements a streaming system to surface real-time updates.\\n4. There are different stream modes available, including agent progress and LLM tokens.\\n5. Custom updates can be streamed by using the `streaming` parameter in the `ChatOpenAI` model.\\n6. Some chat model integrations support the `streaming` parameter, while others require using `disable_streaming=True`.\\n7. LangGraph streaming offers advanced options, including values, debug modes, and subgraph streaming.\\n\\nThis information provides a comprehensive overview of streaming in langchain and its various applications, including its importance, implementation, and customization options.\", additional_kwargs={}, response_metadata={}, id='b5a55997-fd2b-4445-b0ab-c5238c1e8da7')]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.invoke({\"messages\":\"streaming in langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c98233",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'invoke'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res=\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mstreaming in langchain\u001b[39m\u001b[33m\"\u001b[39m}})\n\u001b[32m      2\u001b[39m res[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m].content\n",
      "\u001b[31mAttributeError\u001b[39m: 'function' object has no attribute 'invoke'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db7d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='hi how are you', additional_kwargs={}, response_metadata={}, id='bb805a05-1918-45d9-81bf-b0ec4375c5cc')]}\n",
      "\u001b[1m[updates]\u001b[0m {'model': {'messages': [AIMessage(content=\"I'm functioning properly, thanks for asking. How can I assist you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 228, 'total_tokens': 245, 'completion_time': 0.03707645, 'completion_tokens_details': None, 'prompt_time': 0.019042261, 'prompt_tokens_details': None, 'queue_time': 0.050307079, 'total_time': 0.056118711}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b4db8-25f3-77e2-a909-19b8e6b86e2e-0', usage_metadata={'input_tokens': 228, 'output_tokens': 17, 'total_tokens': 245})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='hi how are you', additional_kwargs={}, response_metadata={}, id='bb805a05-1918-45d9-81bf-b0ec4375c5cc'), AIMessage(content=\"I'm functioning properly, thanks for asking. How can I assist you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 228, 'total_tokens': 245, 'completion_time': 0.03707645, 'completion_tokens_details': None, 'prompt_time': 0.019042261, 'prompt_tokens_details': None, 'queue_time': 0.050307079, 'total_time': 0.056118711}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b4db8-25f3-77e2-a909-19b8e6b86e2e-0', usage_metadata={'input_tokens': 228, 'output_tokens': 17, 'total_tokens': 245})]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hi how are you', additional_kwargs={}, response_metadata={}, id='bb805a05-1918-45d9-81bf-b0ec4375c5cc'),\n",
       "  HumanMessage(content=\"I'm functioning properly, thanks for asking. How can I assist you today?\", additional_kwargs={}, response_metadata={}, id='b0917942-f00f-4181-8f8b-42f122a559e6')]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.invoke({\"messages\":\"hi how are you\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7868d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGs In Depth (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
